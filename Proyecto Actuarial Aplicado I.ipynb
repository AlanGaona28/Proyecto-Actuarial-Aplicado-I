{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4eb24b0",
   "metadata": {},
   "source": [
    "# Prácticum I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7d5090",
   "metadata": {},
   "source": [
    "Nota: Tanto el código como los comentarios están en inglés."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1457c146",
   "metadata": {},
   "source": [
    "The use of machine learning models in financial markets has recently gained relevance due to their ability to identify patterns in historical data and improve investment decision-making. This project aims to develop a machine learning model to predict the direction of a financial asset’s price within a given interval, analyzing the relationship between technical indicators and market behavior. To that end, public data containing two years of historical information in one-minute intervals will be used, cleaned, and transformed using financial indicators.\n",
    "\n",
    "Likewise, various machine learning models will be evaluated to determine their accuracy in price prediction, seeking to identify the optimal time interval in which the model achieves the highest accuracy. Finally, the study will explore the integration of these models into automated trading strategies, considering their opportunities and limitations in real-world scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7853efe8",
   "metadata": {},
   "source": [
    "## Methodolgy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1a7069",
   "metadata": {},
   "source": [
    "The methodology of the project goes as follows:\n",
    "\n",
    "1. Download the historical stock price data, as well as several financial indicators, from the Alpha Vantage API in 1 minute intervals. The downloaded data will include:\n",
    "    - Open, High, Low, Close, and Volume for 1 minute intervals.\n",
    "    - Simple Moving Average for 10 periods.\n",
    "    - Exponential Moving Average for 10 periods.\n",
    "    - Volume-Weighted Average Price.\n",
    "    - Moving Average Convergence/Divergence for 12, 26, and 9 periods for the corresponding parameters.\n",
    "    - Stochastic Oscillator for 14 periods with a 3 period moving average.\n",
    "    - Momentum for 1 period.\n",
    "    - Bollinger Bands for 20 periods.\n",
    "    - Relative Strength Index for 14 periods.\n",
    "2. Feature engineering of the dataset to create a target variable indicating if the price increases or decreases in the next interval. The intervals used for this will be:\n",
    "    - 1 minute\n",
    "    - 3 minutes\n",
    "    - 5 minutes\n",
    "    - 10 minutes\n",
    "    - 15 minutes\n",
    "    - 30 minutes\n",
    "3. Create and train a machine learning model to predict the targer variable. The machine learning models used will be:\n",
    "    - Decision Tree\n",
    "    - Random Forest\n",
    "    - XGBoost\n",
    "4. Test the machine learning model. The metrics used for testing will include:\n",
    "    - F1-Score\n",
    "    - Recall\n",
    "    - Precision\n",
    "5. Setup a backtesting algorithm to improve the accuracy of the model.\n",
    "\n",
    "(Note: A random state of 77 will be used across the code for reproducibility.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea89493e",
   "metadata": {},
   "source": [
    "## 1. Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44348993",
   "metadata": {},
   "source": [
    "The following libraries will be used for the creation of the machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeb7321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Import datasets\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# Runtime\n",
    "import time\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import classification_report, f1_score, make_scorer\n",
    "\n",
    "# Hyperparameters fine-tuning\n",
    "import optuna\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d226557",
   "metadata": {},
   "source": [
    "## 2. Import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0fc9d7",
   "metadata": {},
   "source": [
    "The first step is to download the historical stock price data and financial indicators from the Alpha Vantage API, saving it in csv format for future use. Several functions have been created to simplify the download process of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdeb74b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_url(function: str, symbol: str, interval: str, year: str, month: str, apikey: str, time_period: str, series_type: str, fastkperiod: str, fastdperiod: str) -> str:\n",
    "    \"\"\"\n",
    "    This function creates the url to fetch the data from the Alpha Vantage API.\n",
    "\n",
    "    Args:\n",
    "        function (str): The Alpha Vantage API function.\n",
    "        symbol (str): The stock ticker to query.\n",
    "        interval (str): Time interval for the data.\n",
    "        year (str): Year to fetch data for.\n",
    "        month (str): Month to fetch data for.\n",
    "        apikey (str): Your Alpha Vantage API key.\n",
    "        time_period (str): Time period interval to apply the function.\n",
    "        series_type (str): Series type selected to fetch data.\n",
    "        fastkperiod (str): Time period of the fastk moving average.\n",
    "        fastdperiod (str): Time period of the fastd moving average.\n",
    "\n",
    "    Returns:\n",
    "        str: The complete url to fetch the data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Functions categorised by their parameters\n",
    "    functions_1 = [\"TIME_SERIES_INTRADAY\"]                              # requires outputsize\n",
    "    functions_2 = [\"SMA\", \"EMA\", \"MOM\", \"ROC\", \"BBANDS\", \"RSI\"]         # requires time_period\n",
    "    functions_3 = [\"SMA\", \"EMA\", \"MACD\", \"MOM\", \"ROC\", \"BBANDS\", \"RSI\"] # requires series_type\n",
    "    functions_4 = [\"STOCHF\"]                                            # requiers fastkperiod and fastdperiod\n",
    "\n",
    "    # Create url to fetch data\n",
    "    # Initial parameters for all url\n",
    "    url = (\n",
    "        \"https://www.alphavantage.co/query?function=\" + function + \n",
    "        \"&symbol=\" + symbol + \n",
    "        \"&interval=\" + interval + \n",
    "        \"&month=\" + year + \"-\" + month\n",
    "    )\n",
    "    \n",
    "    # Parameters depending on the function\n",
    "    if function in functions_1:\n",
    "        url = url + \"&outputsize=full\"\n",
    "    \n",
    "    if function in functions_2:\n",
    "        url = url + \"&time_period=\" + time_period\n",
    "\n",
    "    if function in functions_3:\n",
    "        url = url + \"&series_type=\" + series_type\n",
    "\n",
    "    if function in functions_4:\n",
    "        url = url + \"&fastkperiod=\" + fastkperiod + \"&fastdperiod=\" + fastdperiod\n",
    "\n",
    "    # API keys used\n",
    "    # 5EGL4AETQO7XFE3A Alan\n",
    "    # AV63J01W587OXBRX Fong\n",
    "    url = url + \"&apikey=\" + apikey\n",
    "\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ded2697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(url: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetches data from the given API URL, extracts the relevant time series data, and converts it into a Pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        url (str): The API url to fetch the data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the extracted time series data, indexed by the original keys.\n",
    "    \"\"\"\n",
    "\n",
    "    # Request data from the API\n",
    "    r = requests.get(url)\n",
    "    data = r.json()\n",
    "\n",
    "    if \"Information\" in data:\n",
    "        print(\"Error:\", data[\"Information\"])\n",
    "    else:\n",
    "        print(\"API data retrieved successfully\")\n",
    "\n",
    "        # Extract the data of the JSON\n",
    "        key = list(data.keys())[1]\n",
    "        function_data = data.get(key, {})\n",
    "\n",
    "        # Convert the Time Series data into a DataFrame\n",
    "        df = pd.DataFrame.from_dict(function_data, orient=\"index\")\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2599def4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(stock: pd.DataFrame, function: str, symbol: str) -> None:\n",
    "    \"\"\"\n",
    "    This function saves the stock data for the function as a CSV\n",
    "\n",
    "    Args:\n",
    "        stock (pd.DataFrame): The combined DataFrame containing the fetched data.\n",
    "        function (str): The Alpha Vantage API function.\n",
    "        symbol (str): The stock ticker to query.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create directory to save file\n",
    "    DATA_PATH = symbol\n",
    "    if not os.path.exists(DATA_PATH):\n",
    "        os.makedirs(DATA_PATH)\n",
    "\n",
    "    # Save the combined DataFrame to a CSV file\n",
    "    stock.to_csv(symbol + \"/\" + symbol + \" \" + function + \".csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0de5e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_download(function: str, symbol: str, interval: str, years: list, months: list, apikey: str, time_period: str, series_type: str, fastkperiod: str, fastdperiod: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Download stock data from Alpha Vantage for specific years and months and save it as a CSV.\n",
    "\n",
    "    This function fetches stock data from the Alpha Vantage API for the specified parameters.\n",
    "    The data is combined into a Pandas DataFrame and saved to a CSV file.\n",
    "\n",
    "    Note: The function is maxed out to 25 calls for each Key.\n",
    "\n",
    "    Args:\n",
    "        function (str): The Alpha Vantage API function.\n",
    "        symbol (str): The stock ticker to query.\n",
    "        interval (str): Time interval for the data.\n",
    "        years (list of str): List of years to fetch data for.\n",
    "        months (list of str): List of months to fetch data for.\n",
    "        apikey (str): Your Alpha Vantage API key.\n",
    "        time_period (str): Time period interval to apply the function.\n",
    "        series_type (str): Series type selected to fetch data.\n",
    "        fastkperiod (str): Time period of the fastk moving average.\n",
    "        fastdperiod (str): Time period of the fastd moving average.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The combined DataFrame containing the fetched data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize an empty DataFrame to store the combined data\n",
    "    stock = pd.DataFrame()\n",
    "\n",
    "    # Fetch data\n",
    "    for year in years:\n",
    "        for month in months:\n",
    "            \n",
    "            # Create url\n",
    "            url = create_url(function, symbol, interval, year, month, apikey, time_period, series_type, fastkperiod, fastdperiod)\n",
    "            \n",
    "            # Request data from the API and asign to DataFrame\n",
    "            df = fetch_data(url)\n",
    "\n",
    "            # Append to the main DataFrame\n",
    "            stock = pd.concat([df, stock])\n",
    "    \n",
    "    # Reorder the index and adjust format\n",
    "    stock.reset_index(drop=False, inplace=True)\n",
    "    stock.sort_index(ascending=False, inplace=True)\n",
    "    stock[\"index\"] = pd.to_datetime(stock[\"index\"]).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    stock.set_index(\"index\", inplace=True)\n",
    "\n",
    "    save_data(stock, function, symbol)\n",
    "\n",
    "    return stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3f5d77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(function: str, symbol: str, interval: str, years: list, months: list, apikey: str, time_period: str = \"\", series_type: str = \"\", fastkperiod: str = \"\", fastdperiod: str = \"\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read stock data from Alpha Vantage for specific functions, years and months.\n",
    "\n",
    "    Args:\n",
    "        function (str): The Alpha Vantage API function.\n",
    "        symbol (str): The stock ticker to query.\n",
    "        interval (str): Time interval for the data.\n",
    "        years (list of str): List of years to fetch data for.\n",
    "        months (list of str): List of months to fetch data for.\n",
    "        apikey (str): Your Alpha Vantage API key.\n",
    "        time_period (str, optional): Time period interval to apply the function. Defalt value is empty string.\n",
    "        series_type (str, optional): Series type selected to fetch data. Defalt value is empty string.\n",
    "        fastkperiod (str, optional): Time period of the fastk moving average. Defalt value is empty string.\n",
    "        fastdperiod (str, optional): Time period of the fastd moving average. Defalt value is empty string.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame containing the data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Data path for the data file\n",
    "    DATA_PATH = symbol + \"/\" + symbol + \" \" + function + \".csv\"\n",
    "\n",
    "    # Confirms if the file exists\n",
    "    if os.path.exists(DATA_PATH):\n",
    "        # Reads the csv file into a DataFrame\n",
    "        print(\"Function file exists\")\n",
    "        stock = pd.read_csv(DATA_PATH, index_col=0)\n",
    "\n",
    "    else:\n",
    "        # Downloads the data from the API\n",
    "        print(\"Creating function file\")\n",
    "        stock = alpha_download(function, symbol, interval, years, months, apikey, time_period, series_type, fastkperiod, fastdperiod)\n",
    "        print(\"Function file created\")\n",
    "        \n",
    "    return stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0d7ed0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_merged_data(symbol: str, interval: str, years: list, months: list, apikey: str, functions: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read stock data from Alpha Vantage for multiple functions, years and months.\n",
    "    \n",
    "    Args:\n",
    "        symbol (str): The stock ticker symbol.\n",
    "        interval (str): The time interval for the stock data.\n",
    "        years (list): A list of years for which data is retrieved.\n",
    "        months (list): A list of months for which data is retrieved.\n",
    "        apikey (str): The API key for accessing the data.\n",
    "        functions (list): A list of tuples where each tuple contains:\n",
    "            - function (str): The Alpha Vantage API function.\n",
    "            - Optional parameters for the function:\n",
    "                - time_period (str): Time period interval to apply the function.\n",
    "                - series_type (str): Series type selected to fetch data.\n",
    "                - fastkperiod (str): Time period of the fastk moving average.\n",
    "                - fastdperiod (str): Time period of the fastd moving average.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A merged DataFrame containing the requested stock data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Data path for the data file\n",
    "    path_symbol = symbol.replace(\"/\", \"_\")\n",
    "\n",
    "    DATA_PATH = path_symbol + \"/\" + path_symbol + \"_\" + interval + \".csv\"\n",
    "\n",
    "    # Confirms if the file exists\n",
    "    if os.path.exists(DATA_PATH):\n",
    "        # Reads the csv file into a DataFrame\n",
    "        print(\"Complete file exists\")\n",
    "        stock = pd.read_csv(DATA_PATH, index_col=0)\n",
    "    \n",
    "    else:\n",
    "        # Merges from previous datasets\n",
    "        print(\"Creating merged file\")\n",
    "\n",
    "        # Initialize an empty DataFrame to store the combined data\n",
    "        stock = None\n",
    "        \n",
    "        # Read the data for each function\n",
    "        for function_name, *optional_params in functions:\n",
    "            df = read_data(function_name, symbol, interval, years, months, apikey, *optional_params)\n",
    "\n",
    "            # Merge the datasets\n",
    "            if stock is None:\n",
    "                stock = df\n",
    "            else:\n",
    "                stock = pd.merge(stock, df, left_index=True, right_index=True)\n",
    "        \n",
    "        # Save the combined DataFrame to a CSV file\n",
    "        stock.to_csv(symbol + \"/\" + symbol + \".csv\", index=True)\n",
    "        print(\"Merged file created\")\n",
    "    \n",
    "    # Ensure the DataFrame type is numeric\n",
    "    stock = stock.apply(pd.to_numeric)\n",
    "\n",
    "    # Confirms that the complete file is imported\n",
    "    print(\"Complete file imported\")\n",
    "\n",
    "    return stock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b22bad",
   "metadata": {},
   "source": [
    "The following technical indicators will be included, with the follwing parameters for each one:\n",
    "- Open, High, Low, Close, and Volume (only for stocks) for 1 minute intervals.\n",
    "- Simple Moving Average for 10 periods.\n",
    "- Exponential Moving Average for 10 periods.\n",
    "- Volume-Weighted Average Price.\n",
    "- Moving Average Convergence/Divergence for 12, 26, and 9 periods for the corresponding parameters.\n",
    "- Stochastic Oscillator for 14 periods with a 3 period moving average.\n",
    "- Momentum for 1 period.\n",
    "- Rate of Change for 1 period.\n",
    "- Bollinger Bands for 20 periods.\n",
    "- Relative Strength Index for 14 periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be5971c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete file exists\n",
      "Complete file imported\n"
     ]
    }
   ],
   "source": [
    "# Create stock dataset\n",
    "symbol = \"EUR/USD\"\n",
    "interval = \"1min\"\n",
    "years = [\"2022\", \"2023\"]    #[\"2022\", \"2023\"]\n",
    "months = [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]   #[\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]\n",
    "apikey = \"\" # Change when downloading data\n",
    "\n",
    "functions_list = [\n",
    "    (\"TIME_SERIES_INTRADAY\", \"\", \"\"),\n",
    "    (\"SMA\", \"10\", \"close\", \"\", \"\"),\n",
    "    (\"EMA\", \"10\", \"close\", \"\", \"\"),\n",
    "    (\"STOCHF\", \"\", \"\", \"14\", \"3\"),\n",
    "    (\"MOM\", \"1\", \"close\", \"\", \"\"),\n",
    "    (\"BBANDS\", \"20\", \"close\", \"\", \"\"),\n",
    "    (\"RSI\", \"14\", \"close\", \"\", \"\")\n",
    "]\n",
    "\n",
    "stock = read_merged_data(symbol, interval, years, months, apikey, functions_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f788ed",
   "metadata": {},
   "source": [
    "Now, a dataframe containing the merged data from each function for the selected years for the stock has been created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c7ab10",
   "metadata": {},
   "source": [
    "Visualisation of the dataset characteristics and general behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbdf60f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>sma</th>\n",
       "      <th>ema</th>\n",
       "      <th>vwap</th>\n",
       "      <th>macd</th>\n",
       "      <th>macd_signal</th>\n",
       "      <th>macd_hist</th>\n",
       "      <th>fast_k</th>\n",
       "      <th>fast_d</th>\n",
       "      <th>mom</th>\n",
       "      <th>roc</th>\n",
       "      <th>upper_band</th>\n",
       "      <th>middle_band</th>\n",
       "      <th>lower_band</th>\n",
       "      <th>rsi</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-01-01 00:00:00</th>\n",
       "      <td>1.13255</td>\n",
       "      <td>1.13270</td>\n",
       "      <td>1.1325</td>\n",
       "      <td>1.1327</td>\n",
       "      <td>1.13286</td>\n",
       "      <td>1.13284</td>\n",
       "      <td>1.13263</td>\n",
       "      <td>-0.00004</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>-0.00009</td>\n",
       "      <td>19.99285</td>\n",
       "      <td>12.25175</td>\n",
       "      <td>0.00015</td>\n",
       "      <td>0.01324</td>\n",
       "      <td>1.13360</td>\n",
       "      <td>1.13306</td>\n",
       "      <td>1.13253</td>\n",
       "      <td>43.09303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 00:01:00</th>\n",
       "      <td>1.13275</td>\n",
       "      <td>1.13275</td>\n",
       "      <td>1.1325</td>\n",
       "      <td>1.1325</td>\n",
       "      <td>1.13279</td>\n",
       "      <td>1.13278</td>\n",
       "      <td>1.13258</td>\n",
       "      <td>-0.00007</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>-0.00009</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>8.32936</td>\n",
       "      <td>-0.00020</td>\n",
       "      <td>-0.01765</td>\n",
       "      <td>1.13363</td>\n",
       "      <td>1.13304</td>\n",
       "      <td>1.13245</td>\n",
       "      <td>38.15357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 00:02:00</th>\n",
       "      <td>1.13250</td>\n",
       "      <td>1.13270</td>\n",
       "      <td>1.1325</td>\n",
       "      <td>1.1326</td>\n",
       "      <td>1.13274</td>\n",
       "      <td>1.13275</td>\n",
       "      <td>1.13260</td>\n",
       "      <td>-0.00008</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00008</td>\n",
       "      <td>9.99046</td>\n",
       "      <td>9.99444</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.00882</td>\n",
       "      <td>1.13364</td>\n",
       "      <td>1.13302</td>\n",
       "      <td>1.13240</td>\n",
       "      <td>41.74682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 00:03:00</th>\n",
       "      <td>1.13250</td>\n",
       "      <td>1.13270</td>\n",
       "      <td>1.1325</td>\n",
       "      <td>1.1326</td>\n",
       "      <td>1.13272</td>\n",
       "      <td>1.13272</td>\n",
       "      <td>1.13260</td>\n",
       "      <td>-0.00009</td>\n",
       "      <td>-0.00002</td>\n",
       "      <td>-0.00007</td>\n",
       "      <td>9.99046</td>\n",
       "      <td>6.66031</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.13361</td>\n",
       "      <td>1.13298</td>\n",
       "      <td>1.13236</td>\n",
       "      <td>41.74682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 00:04:00</th>\n",
       "      <td>1.13260</td>\n",
       "      <td>1.13270</td>\n",
       "      <td>1.1325</td>\n",
       "      <td>1.1327</td>\n",
       "      <td>1.13270</td>\n",
       "      <td>1.13272</td>\n",
       "      <td>1.13263</td>\n",
       "      <td>-0.00009</td>\n",
       "      <td>-0.00003</td>\n",
       "      <td>-0.00006</td>\n",
       "      <td>23.52034</td>\n",
       "      <td>14.50042</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.00883</td>\n",
       "      <td>1.13358</td>\n",
       "      <td>1.13295</td>\n",
       "      <td>1.13233</td>\n",
       "      <td>45.42834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-30 06:55:00</th>\n",
       "      <td>1.10400</td>\n",
       "      <td>1.10400</td>\n",
       "      <td>1.1038</td>\n",
       "      <td>1.1039</td>\n",
       "      <td>1.10416</td>\n",
       "      <td>1.10412</td>\n",
       "      <td>1.10390</td>\n",
       "      <td>-0.00007</td>\n",
       "      <td>-0.00006</td>\n",
       "      <td>-0.00001</td>\n",
       "      <td>16.65011</td>\n",
       "      <td>13.88337</td>\n",
       "      <td>-0.00010</td>\n",
       "      <td>-0.00906</td>\n",
       "      <td>1.10445</td>\n",
       "      <td>1.10420</td>\n",
       "      <td>1.10394</td>\n",
       "      <td>40.40055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-30 06:56:00</th>\n",
       "      <td>1.10400</td>\n",
       "      <td>1.10410</td>\n",
       "      <td>1.1040</td>\n",
       "      <td>1.1041</td>\n",
       "      <td>1.10414</td>\n",
       "      <td>1.10412</td>\n",
       "      <td>1.10407</td>\n",
       "      <td>-0.00007</td>\n",
       "      <td>-0.00006</td>\n",
       "      <td>-0.00001</td>\n",
       "      <td>49.99007</td>\n",
       "      <td>22.21339</td>\n",
       "      <td>0.00020</td>\n",
       "      <td>0.01812</td>\n",
       "      <td>1.10445</td>\n",
       "      <td>1.10420</td>\n",
       "      <td>1.10394</td>\n",
       "      <td>46.86666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-30 06:57:00</th>\n",
       "      <td>1.10400</td>\n",
       "      <td>1.10400</td>\n",
       "      <td>1.1040</td>\n",
       "      <td>1.1040</td>\n",
       "      <td>1.10413</td>\n",
       "      <td>1.10410</td>\n",
       "      <td>1.10400</td>\n",
       "      <td>-0.00008</td>\n",
       "      <td>-0.00006</td>\n",
       "      <td>-0.00001</td>\n",
       "      <td>33.32009</td>\n",
       "      <td>33.32009</td>\n",
       "      <td>-0.00010</td>\n",
       "      <td>-0.00906</td>\n",
       "      <td>1.10445</td>\n",
       "      <td>1.10418</td>\n",
       "      <td>1.10391</td>\n",
       "      <td>44.27986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-30 06:58:00</th>\n",
       "      <td>1.10400</td>\n",
       "      <td>1.10400</td>\n",
       "      <td>1.1038</td>\n",
       "      <td>1.1039</td>\n",
       "      <td>1.10411</td>\n",
       "      <td>1.10406</td>\n",
       "      <td>1.10390</td>\n",
       "      <td>-0.00009</td>\n",
       "      <td>-0.00007</td>\n",
       "      <td>-0.00002</td>\n",
       "      <td>16.65011</td>\n",
       "      <td>33.32009</td>\n",
       "      <td>-0.00010</td>\n",
       "      <td>-0.00906</td>\n",
       "      <td>1.10446</td>\n",
       "      <td>1.10416</td>\n",
       "      <td>1.10387</td>\n",
       "      <td>41.79552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-30 06:59:00</th>\n",
       "      <td>1.10390</td>\n",
       "      <td>1.10420</td>\n",
       "      <td>1.1039</td>\n",
       "      <td>1.1041</td>\n",
       "      <td>1.10408</td>\n",
       "      <td>1.10407</td>\n",
       "      <td>1.10407</td>\n",
       "      <td>-0.00008</td>\n",
       "      <td>-0.00007</td>\n",
       "      <td>-0.00001</td>\n",
       "      <td>49.99007</td>\n",
       "      <td>33.32009</td>\n",
       "      <td>0.00020</td>\n",
       "      <td>0.01812</td>\n",
       "      <td>1.10446</td>\n",
       "      <td>1.10416</td>\n",
       "      <td>1.10387</td>\n",
       "      <td>48.07078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>739201 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        open     high     low   close      sma      ema  \\\n",
       "datetime                                                                  \n",
       "2022-01-01 00:00:00  1.13255  1.13270  1.1325  1.1327  1.13286  1.13284   \n",
       "2022-01-01 00:01:00  1.13275  1.13275  1.1325  1.1325  1.13279  1.13278   \n",
       "2022-01-01 00:02:00  1.13250  1.13270  1.1325  1.1326  1.13274  1.13275   \n",
       "2022-01-01 00:03:00  1.13250  1.13270  1.1325  1.1326  1.13272  1.13272   \n",
       "2022-01-01 00:04:00  1.13260  1.13270  1.1325  1.1327  1.13270  1.13272   \n",
       "...                      ...      ...     ...     ...      ...      ...   \n",
       "2023-12-30 06:55:00  1.10400  1.10400  1.1038  1.1039  1.10416  1.10412   \n",
       "2023-12-30 06:56:00  1.10400  1.10410  1.1040  1.1041  1.10414  1.10412   \n",
       "2023-12-30 06:57:00  1.10400  1.10400  1.1040  1.1040  1.10413  1.10410   \n",
       "2023-12-30 06:58:00  1.10400  1.10400  1.1038  1.1039  1.10411  1.10406   \n",
       "2023-12-30 06:59:00  1.10390  1.10420  1.1039  1.1041  1.10408  1.10407   \n",
       "\n",
       "                        vwap     macd  macd_signal  macd_hist    fast_k  \\\n",
       "datetime                                                                  \n",
       "2022-01-01 00:00:00  1.13263 -0.00004      0.00005   -0.00009  19.99285   \n",
       "2022-01-01 00:01:00  1.13258 -0.00007      0.00002   -0.00009   0.00000   \n",
       "2022-01-01 00:02:00  1.13260 -0.00008      0.00000   -0.00008   9.99046   \n",
       "2022-01-01 00:03:00  1.13260 -0.00009     -0.00002   -0.00007   9.99046   \n",
       "2022-01-01 00:04:00  1.13263 -0.00009     -0.00003   -0.00006  23.52034   \n",
       "...                      ...      ...          ...        ...       ...   \n",
       "2023-12-30 06:55:00  1.10390 -0.00007     -0.00006   -0.00001  16.65011   \n",
       "2023-12-30 06:56:00  1.10407 -0.00007     -0.00006   -0.00001  49.99007   \n",
       "2023-12-30 06:57:00  1.10400 -0.00008     -0.00006   -0.00001  33.32009   \n",
       "2023-12-30 06:58:00  1.10390 -0.00009     -0.00007   -0.00002  16.65011   \n",
       "2023-12-30 06:59:00  1.10407 -0.00008     -0.00007   -0.00001  49.99007   \n",
       "\n",
       "                       fast_d      mom      roc  upper_band  middle_band  \\\n",
       "datetime                                                                   \n",
       "2022-01-01 00:00:00  12.25175  0.00015  0.01324     1.13360      1.13306   \n",
       "2022-01-01 00:01:00   8.32936 -0.00020 -0.01765     1.13363      1.13304   \n",
       "2022-01-01 00:02:00   9.99444  0.00010  0.00882     1.13364      1.13302   \n",
       "2022-01-01 00:03:00   6.66031  0.00000  0.00000     1.13361      1.13298   \n",
       "2022-01-01 00:04:00  14.50042  0.00010  0.00883     1.13358      1.13295   \n",
       "...                       ...      ...      ...         ...          ...   \n",
       "2023-12-30 06:55:00  13.88337 -0.00010 -0.00906     1.10445      1.10420   \n",
       "2023-12-30 06:56:00  22.21339  0.00020  0.01812     1.10445      1.10420   \n",
       "2023-12-30 06:57:00  33.32009 -0.00010 -0.00906     1.10445      1.10418   \n",
       "2023-12-30 06:58:00  33.32009 -0.00010 -0.00906     1.10446      1.10416   \n",
       "2023-12-30 06:59:00  33.32009  0.00020  0.01812     1.10446      1.10416   \n",
       "\n",
       "                     lower_band       rsi  \n",
       "datetime                                   \n",
       "2022-01-01 00:00:00     1.13253  43.09303  \n",
       "2022-01-01 00:01:00     1.13245  38.15357  \n",
       "2022-01-01 00:02:00     1.13240  41.74682  \n",
       "2022-01-01 00:03:00     1.13236  41.74682  \n",
       "2022-01-01 00:04:00     1.13233  45.42834  \n",
       "...                         ...       ...  \n",
       "2023-12-30 06:55:00     1.10394  40.40055  \n",
       "2023-12-30 06:56:00     1.10394  46.86666  \n",
       "2023-12-30 06:57:00     1.10391  44.27986  \n",
       "2023-12-30 06:58:00     1.10387  41.79552  \n",
       "2023-12-30 06:59:00     1.10387  48.07078  \n",
       "\n",
       "[739201 rows x 18 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the general view of the stock dataframe\n",
    "stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fadf28ca-5a8c-421e-b4ef-cd02def0a895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize stock close prices\n",
    "stock.plot.line(y = \"close\", title = symbol + \" Closing Price\").tick_params(axis = \"x\", labelrotation = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e17bb85",
   "metadata": {},
   "source": [
    "## 3. Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833e12eb",
   "metadata": {},
   "source": [
    "Different target variables will be created for different time intervals. The objective is to evaluate the performance in the prediction across different intervals. The intervals used will be:\n",
    "- 1 minute\n",
    "- 3 minutes\n",
    "- 5 minutes\n",
    "- 10 minutes\n",
    "- 15 minutes\n",
    "- 30 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74f25d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_targets(stock: pd.DataFrame, intervals: list, predictors: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a DataFrame including multiple target columns indicating whether the close price is higher after a given interval.\n",
    "    Then appends the specified predictors from the stock DataFrame.\n",
    "\n",
    "    Args:\n",
    "        stock (pd.DataFrame): DataFrame containing the stock information.\n",
    "        intervals (list): A list of integers representing the intervals after which to check if the close price increased.\n",
    "        predictors (list): A list of column names from `stock` to be joined as additional features in the output DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the newly created target columns and the specified predictors.\n",
    "    \"\"\"\n",
    "\n",
    "    # Base stock DataFrame with 'Actual_close'\n",
    "    data = stock[[\"close\"]].rename(columns={\"close\": \"Actual_close\"})\n",
    "\n",
    "    # Create target columns for each interval\n",
    "    for minutes in intervals:\n",
    "        col_name = f\"Target_{minutes}\"\n",
    "\n",
    "        future_close = stock[\"close\"].shift(-minutes)\n",
    "        data[col_name] = future_close > (stock[\"close\"])\n",
    "        data[col_name] = data[col_name].astype(int)\n",
    "\n",
    "    # Join the predictors from the original stock DataFrame\n",
    "    data = data.join(stock[predictors])\n",
    "\n",
    "    # Drop the initial rows where rolling calculations yield NaN\n",
    "    data = data.iloc[max(intervals):]\n",
    "\n",
    "    # Drop 'Actual_close' since it's no longer needed\n",
    "    data.drop(\"Actual_close\", axis=1, inplace=True)\n",
    "\n",
    "    # Reset the index and drop any remaining NaNs\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    data.dropna(axis=0, inplace=True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b625c0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the intervals for the target columns\n",
    "intervals = [1, 3, 5, 10, 15, 30]\n",
    "Targets = [f\"Target_{minutes}\" for minutes in intervals]\n",
    "\n",
    "# Specify the columns used as predictors\n",
    "predictors = stock.columns.tolist()\n",
    "\n",
    "data = create_targets(stock, intervals, predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98b09db4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target_1</th>\n",
       "      <th>Target_3</th>\n",
       "      <th>Target_5</th>\n",
       "      <th>Target_10</th>\n",
       "      <th>Target_15</th>\n",
       "      <th>Target_30</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>sma</th>\n",
       "      <th>ema</th>\n",
       "      <th>vwap</th>\n",
       "      <th>macd</th>\n",
       "      <th>macd_signal</th>\n",
       "      <th>macd_hist</th>\n",
       "      <th>fast_k</th>\n",
       "      <th>fast_d</th>\n",
       "      <th>mom</th>\n",
       "      <th>roc</th>\n",
       "      <th>upper_band</th>\n",
       "      <th>middle_band</th>\n",
       "      <th>lower_band</th>\n",
       "      <th>rsi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.13285</td>\n",
       "      <td>1.13285</td>\n",
       "      <td>1.13265</td>\n",
       "      <td>1.1327</td>\n",
       "      <td>1.13253</td>\n",
       "      <td>1.13262</td>\n",
       "      <td>1.13273</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>-0.00007</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>71.42371</td>\n",
       "      <td>90.47457</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>-0.02648</td>\n",
       "      <td>1.13315</td>\n",
       "      <td>1.13250</td>\n",
       "      <td>1.13185</td>\n",
       "      <td>53.33065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.13260</td>\n",
       "      <td>1.13260</td>\n",
       "      <td>1.13240</td>\n",
       "      <td>1.1324</td>\n",
       "      <td>1.13255</td>\n",
       "      <td>1.13258</td>\n",
       "      <td>1.13247</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00006</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>42.85876</td>\n",
       "      <td>71.42749</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>-0.02648</td>\n",
       "      <td>1.13306</td>\n",
       "      <td>1.13247</td>\n",
       "      <td>1.13187</td>\n",
       "      <td>45.69384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.13245</td>\n",
       "      <td>1.13290</td>\n",
       "      <td>1.13245</td>\n",
       "      <td>1.1329</td>\n",
       "      <td>1.13261</td>\n",
       "      <td>1.13264</td>\n",
       "      <td>1.13275</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>-0.00004</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>90.47457</td>\n",
       "      <td>68.25235</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.04415</td>\n",
       "      <td>1.13306</td>\n",
       "      <td>1.13247</td>\n",
       "      <td>1.13187</td>\n",
       "      <td>56.79914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.13295</td>\n",
       "      <td>1.13315</td>\n",
       "      <td>1.13290</td>\n",
       "      <td>1.1331</td>\n",
       "      <td>1.13269</td>\n",
       "      <td>1.13272</td>\n",
       "      <td>1.13305</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>-0.00002</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>95.65668</td>\n",
       "      <td>76.33000</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.01766</td>\n",
       "      <td>1.13310</td>\n",
       "      <td>1.13248</td>\n",
       "      <td>1.13185</td>\n",
       "      <td>60.29737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.13305</td>\n",
       "      <td>1.13305</td>\n",
       "      <td>1.13285</td>\n",
       "      <td>1.1329</td>\n",
       "      <td>1.13275</td>\n",
       "      <td>1.13275</td>\n",
       "      <td>1.13293</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>78.26267</td>\n",
       "      <td>88.13131</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>-0.01765</td>\n",
       "      <td>1.13315</td>\n",
       "      <td>1.13249</td>\n",
       "      <td>1.13184</td>\n",
       "      <td>55.46091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.13300</td>\n",
       "      <td>1.13300</td>\n",
       "      <td>1.13285</td>\n",
       "      <td>1.1329</td>\n",
       "      <td>1.13280</td>\n",
       "      <td>1.13278</td>\n",
       "      <td>1.13292</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>78.26267</td>\n",
       "      <td>84.06068</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.13319</td>\n",
       "      <td>1.13252</td>\n",
       "      <td>1.13184</td>\n",
       "      <td>55.46091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.13290</td>\n",
       "      <td>1.13290</td>\n",
       "      <td>1.13280</td>\n",
       "      <td>1.1328</td>\n",
       "      <td>1.13283</td>\n",
       "      <td>1.13278</td>\n",
       "      <td>1.13283</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>63.15723</td>\n",
       "      <td>73.22753</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>-0.00883</td>\n",
       "      <td>1.13322</td>\n",
       "      <td>1.13254</td>\n",
       "      <td>1.13185</td>\n",
       "      <td>52.99594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.13285</td>\n",
       "      <td>1.13290</td>\n",
       "      <td>1.13285</td>\n",
       "      <td>1.1329</td>\n",
       "      <td>1.13286</td>\n",
       "      <td>1.13280</td>\n",
       "      <td>1.13288</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>73.68553</td>\n",
       "      <td>71.70181</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.00883</td>\n",
       "      <td>1.13326</td>\n",
       "      <td>1.13257</td>\n",
       "      <td>1.13187</td>\n",
       "      <td>55.14297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.13290</td>\n",
       "      <td>1.13300</td>\n",
       "      <td>1.13290</td>\n",
       "      <td>1.1330</td>\n",
       "      <td>1.13286</td>\n",
       "      <td>1.13284</td>\n",
       "      <td>1.13297</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>82.35624</td>\n",
       "      <td>73.06634</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.00883</td>\n",
       "      <td>1.13330</td>\n",
       "      <td>1.13261</td>\n",
       "      <td>1.13192</td>\n",
       "      <td>57.24610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.13300</td>\n",
       "      <td>1.13320</td>\n",
       "      <td>1.13290</td>\n",
       "      <td>1.1332</td>\n",
       "      <td>1.13288</td>\n",
       "      <td>1.13291</td>\n",
       "      <td>1.13310</td>\n",
       "      <td>0.00012</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>85.34726</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.01766</td>\n",
       "      <td>1.13335</td>\n",
       "      <td>1.13267</td>\n",
       "      <td>1.13199</td>\n",
       "      <td>61.16752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.13310</td>\n",
       "      <td>1.13330</td>\n",
       "      <td>1.13310</td>\n",
       "      <td>1.1333</td>\n",
       "      <td>1.13294</td>\n",
       "      <td>1.13298</td>\n",
       "      <td>1.13323</td>\n",
       "      <td>0.00015</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>94.11875</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.00882</td>\n",
       "      <td>1.13339</td>\n",
       "      <td>1.13274</td>\n",
       "      <td>1.13208</td>\n",
       "      <td>62.99305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.13320</td>\n",
       "      <td>1.13340</td>\n",
       "      <td>1.13320</td>\n",
       "      <td>1.1334</td>\n",
       "      <td>1.13304</td>\n",
       "      <td>1.13305</td>\n",
       "      <td>1.13333</td>\n",
       "      <td>0.00017</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.00883</td>\n",
       "      <td>1.13346</td>\n",
       "      <td>1.13280</td>\n",
       "      <td>1.13213</td>\n",
       "      <td>64.77833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.13340</td>\n",
       "      <td>1.13360</td>\n",
       "      <td>1.13340</td>\n",
       "      <td>1.1336</td>\n",
       "      <td>1.13311</td>\n",
       "      <td>1.13315</td>\n",
       "      <td>1.13353</td>\n",
       "      <td>0.00021</td>\n",
       "      <td>0.00012</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.01765</td>\n",
       "      <td>1.13357</td>\n",
       "      <td>1.13286</td>\n",
       "      <td>1.13215</td>\n",
       "      <td>68.09357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.13350</td>\n",
       "      <td>1.13350</td>\n",
       "      <td>1.13330</td>\n",
       "      <td>1.1334</td>\n",
       "      <td>1.13314</td>\n",
       "      <td>1.13320</td>\n",
       "      <td>1.13340</td>\n",
       "      <td>0.00022</td>\n",
       "      <td>0.00014</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>83.33002</td>\n",
       "      <td>94.44334</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>-0.01765</td>\n",
       "      <td>1.13362</td>\n",
       "      <td>1.13292</td>\n",
       "      <td>1.13221</td>\n",
       "      <td>61.82650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.13340</td>\n",
       "      <td>1.13340</td>\n",
       "      <td>1.13330</td>\n",
       "      <td>1.1333</td>\n",
       "      <td>1.13318</td>\n",
       "      <td>1.13322</td>\n",
       "      <td>1.13333</td>\n",
       "      <td>0.00021</td>\n",
       "      <td>0.00015</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>74.99503</td>\n",
       "      <td>86.10835</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>-0.00882</td>\n",
       "      <td>1.13363</td>\n",
       "      <td>1.13296</td>\n",
       "      <td>1.13230</td>\n",
       "      <td>58.90718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Target_1  Target_3  Target_5  Target_10  Target_15  Target_30     open  \\\n",
       "0          0         1         1          1          1          1  1.13285   \n",
       "1          1         1         1          1          1          1  1.13260   \n",
       "2          1         0         0          1          1          1  1.13245   \n",
       "3          0         0         0          1          1          1  1.13295   \n",
       "4          0         0         1          1          1          1  1.13305   \n",
       "5          0         1         1          1          1          1  1.13300   \n",
       "6          1         1         1          1          1          1  1.13290   \n",
       "7          1         1         1          1          1          1  1.13285   \n",
       "8          1         1         1          1          1          1  1.13290   \n",
       "9          1         1         1          1          1          1  1.13300   \n",
       "10         1         1         1          1          1          1  1.13310   \n",
       "11         1         0         1          1          1          1  1.13320   \n",
       "12         0         0         1          1          1          1  1.13340   \n",
       "13         0         1         1          1          1          1  1.13350   \n",
       "14         1         1         1          1          1          1  1.13340   \n",
       "\n",
       "       high      low   close      sma      ema     vwap     macd  macd_signal  \\\n",
       "0   1.13285  1.13265  1.1327  1.13253  1.13262  1.13273  0.00001     -0.00007   \n",
       "1   1.13260  1.13240  1.1324  1.13255  1.13258  1.13247 -0.00000     -0.00006   \n",
       "2   1.13290  1.13245  1.1329  1.13261  1.13264  1.13275  0.00003     -0.00004   \n",
       "3   1.13315  1.13290  1.1331  1.13269  1.13272  1.13305  0.00006     -0.00002   \n",
       "4   1.13305  1.13285  1.1329  1.13275  1.13275  1.13293  0.00007     -0.00000   \n",
       "5   1.13300  1.13285  1.1329  1.13280  1.13278  1.13292  0.00008      0.00002   \n",
       "6   1.13290  1.13280  1.1328  1.13283  1.13278  1.13283  0.00008      0.00003   \n",
       "7   1.13290  1.13285  1.1329  1.13286  1.13280  1.13288  0.00009      0.00004   \n",
       "8   1.13300  1.13290  1.1330  1.13286  1.13284  1.13297  0.00010      0.00005   \n",
       "9   1.13320  1.13290  1.1332  1.13288  1.13291  1.13310  0.00012      0.00007   \n",
       "10  1.13330  1.13310  1.1333  1.13294  1.13298  1.13323  0.00015      0.00008   \n",
       "11  1.13340  1.13320  1.1334  1.13304  1.13305  1.13333  0.00017      0.00010   \n",
       "12  1.13360  1.13340  1.1336  1.13311  1.13315  1.13353  0.00021      0.00012   \n",
       "13  1.13350  1.13330  1.1334  1.13314  1.13320  1.13340  0.00022      0.00014   \n",
       "14  1.13340  1.13330  1.1333  1.13318  1.13322  1.13333  0.00021      0.00015   \n",
       "\n",
       "    macd_hist     fast_k     fast_d     mom      roc  upper_band  middle_band  \\\n",
       "0     0.00008   71.42371   90.47457 -0.0003 -0.02648     1.13315      1.13250   \n",
       "1     0.00005   42.85876   71.42749 -0.0003 -0.02648     1.13306      1.13247   \n",
       "2     0.00006   90.47457   68.25235  0.0005  0.04415     1.13306      1.13247   \n",
       "3     0.00008   95.65668   76.33000  0.0002  0.01766     1.13310      1.13248   \n",
       "4     0.00007   78.26267   88.13131 -0.0002 -0.01765     1.13315      1.13249   \n",
       "5     0.00007   78.26267   84.06068  0.0000  0.00000     1.13319      1.13252   \n",
       "6     0.00005   63.15723   73.22753 -0.0001 -0.00883     1.13322      1.13254   \n",
       "7     0.00005   73.68553   71.70181  0.0001  0.00883     1.13326      1.13257   \n",
       "8     0.00005   82.35624   73.06634  0.0001  0.00883     1.13330      1.13261   \n",
       "9     0.00006  100.00000   85.34726  0.0002  0.01766     1.13335      1.13267   \n",
       "10    0.00006  100.00000   94.11875  0.0001  0.00882     1.13339      1.13274   \n",
       "11    0.00007  100.00000  100.00000  0.0001  0.00883     1.13346      1.13280   \n",
       "12    0.00009  100.00000  100.00000  0.0002  0.01765     1.13357      1.13286   \n",
       "13    0.00008   83.33002   94.44334 -0.0002 -0.01765     1.13362      1.13292   \n",
       "14    0.00006   74.99503   86.10835 -0.0001 -0.00882     1.13363      1.13296   \n",
       "\n",
       "    lower_band       rsi  \n",
       "0      1.13185  53.33065  \n",
       "1      1.13187  45.69384  \n",
       "2      1.13187  56.79914  \n",
       "3      1.13185  60.29737  \n",
       "4      1.13184  55.46091  \n",
       "5      1.13184  55.46091  \n",
       "6      1.13185  52.99594  \n",
       "7      1.13187  55.14297  \n",
       "8      1.13192  57.24610  \n",
       "9      1.13199  61.16752  \n",
       "10     1.13208  62.99305  \n",
       "11     1.13213  64.77833  \n",
       "12     1.13215  68.09357  \n",
       "13     1.13221  61.82650  \n",
       "14     1.13230  58.90718  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the first instances of the new data dataframe\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "data.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a6cdf03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target_1</th>\n",
       "      <th>Target_3</th>\n",
       "      <th>Target_5</th>\n",
       "      <th>Target_10</th>\n",
       "      <th>Target_15</th>\n",
       "      <th>Target_30</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>sma</th>\n",
       "      <th>ema</th>\n",
       "      <th>vwap</th>\n",
       "      <th>macd</th>\n",
       "      <th>macd_signal</th>\n",
       "      <th>macd_hist</th>\n",
       "      <th>fast_k</th>\n",
       "      <th>fast_d</th>\n",
       "      <th>mom</th>\n",
       "      <th>roc</th>\n",
       "      <th>upper_band</th>\n",
       "      <th>middle_band</th>\n",
       "      <th>lower_band</th>\n",
       "      <th>rsi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>739156</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.10430</td>\n",
       "      <td>1.10440</td>\n",
       "      <td>1.1042</td>\n",
       "      <td>1.1044</td>\n",
       "      <td>1.10424</td>\n",
       "      <td>1.10424</td>\n",
       "      <td>1.10433</td>\n",
       "      <td>-0.00006</td>\n",
       "      <td>-0.00012</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>80.00000</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.01812</td>\n",
       "      <td>1.10440</td>\n",
       "      <td>1.10415</td>\n",
       "      <td>1.10390</td>\n",
       "      <td>53.91597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739157</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.10440</td>\n",
       "      <td>1.10440</td>\n",
       "      <td>1.1042</td>\n",
       "      <td>1.1043</td>\n",
       "      <td>1.10425</td>\n",
       "      <td>1.10425</td>\n",
       "      <td>1.10430</td>\n",
       "      <td>-0.00005</td>\n",
       "      <td>-0.00010</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>80.00000</td>\n",
       "      <td>80.00000</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>-0.00906</td>\n",
       "      <td>1.10441</td>\n",
       "      <td>1.10415</td>\n",
       "      <td>1.10389</td>\n",
       "      <td>50.09284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739158</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.10420</td>\n",
       "      <td>1.10420</td>\n",
       "      <td>1.1040</td>\n",
       "      <td>1.1041</td>\n",
       "      <td>1.10423</td>\n",
       "      <td>1.10422</td>\n",
       "      <td>1.10410</td>\n",
       "      <td>-0.00006</td>\n",
       "      <td>-0.00009</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>40.00000</td>\n",
       "      <td>73.33333</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>-0.01811</td>\n",
       "      <td>1.10441</td>\n",
       "      <td>1.10415</td>\n",
       "      <td>1.10389</td>\n",
       "      <td>43.45594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739159</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.10415</td>\n",
       "      <td>1.10415</td>\n",
       "      <td>1.1040</td>\n",
       "      <td>1.1041</td>\n",
       "      <td>1.10422</td>\n",
       "      <td>1.10420</td>\n",
       "      <td>1.10408</td>\n",
       "      <td>-0.00006</td>\n",
       "      <td>-0.00009</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>40.00000</td>\n",
       "      <td>53.33333</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.10441</td>\n",
       "      <td>1.10416</td>\n",
       "      <td>1.10390</td>\n",
       "      <td>43.45594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739160</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.10430</td>\n",
       "      <td>1.10440</td>\n",
       "      <td>1.1042</td>\n",
       "      <td>1.1044</td>\n",
       "      <td>1.10425</td>\n",
       "      <td>1.10424</td>\n",
       "      <td>1.10433</td>\n",
       "      <td>-0.00004</td>\n",
       "      <td>-0.00008</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>60.00000</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.02718</td>\n",
       "      <td>1.10444</td>\n",
       "      <td>1.10417</td>\n",
       "      <td>1.10390</td>\n",
       "      <td>54.04749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739161</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.10440</td>\n",
       "      <td>1.10440</td>\n",
       "      <td>1.1043</td>\n",
       "      <td>1.1043</td>\n",
       "      <td>1.10425</td>\n",
       "      <td>1.10425</td>\n",
       "      <td>1.10433</td>\n",
       "      <td>-0.00003</td>\n",
       "      <td>-0.00007</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>75.00000</td>\n",
       "      <td>71.66667</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>-0.00906</td>\n",
       "      <td>1.10446</td>\n",
       "      <td>1.10418</td>\n",
       "      <td>1.10390</td>\n",
       "      <td>50.64224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739162</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.10420</td>\n",
       "      <td>1.10430</td>\n",
       "      <td>1.1041</td>\n",
       "      <td>1.1041</td>\n",
       "      <td>1.10424</td>\n",
       "      <td>1.10422</td>\n",
       "      <td>1.10417</td>\n",
       "      <td>-0.00004</td>\n",
       "      <td>-0.00006</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>25.00000</td>\n",
       "      <td>66.66667</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>-0.01811</td>\n",
       "      <td>1.10446</td>\n",
       "      <td>1.10418</td>\n",
       "      <td>1.10390</td>\n",
       "      <td>44.59111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739163</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.10420</td>\n",
       "      <td>1.10430</td>\n",
       "      <td>1.1042</td>\n",
       "      <td>1.1043</td>\n",
       "      <td>1.10425</td>\n",
       "      <td>1.10423</td>\n",
       "      <td>1.10427</td>\n",
       "      <td>-0.00003</td>\n",
       "      <td>-0.00006</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>75.00000</td>\n",
       "      <td>58.33333</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.01812</td>\n",
       "      <td>1.10447</td>\n",
       "      <td>1.10419</td>\n",
       "      <td>1.10390</td>\n",
       "      <td>50.90820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739164</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.10420</td>\n",
       "      <td>1.10420</td>\n",
       "      <td>1.1040</td>\n",
       "      <td>1.1041</td>\n",
       "      <td>1.10423</td>\n",
       "      <td>1.10421</td>\n",
       "      <td>1.10410</td>\n",
       "      <td>-0.00004</td>\n",
       "      <td>-0.00005</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>25.00000</td>\n",
       "      <td>41.66667</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>-0.01811</td>\n",
       "      <td>1.10445</td>\n",
       "      <td>1.10420</td>\n",
       "      <td>1.10394</td>\n",
       "      <td>45.34127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739165</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.10410</td>\n",
       "      <td>1.10410</td>\n",
       "      <td>1.1040</td>\n",
       "      <td>1.1040</td>\n",
       "      <td>1.10421</td>\n",
       "      <td>1.10417</td>\n",
       "      <td>1.10403</td>\n",
       "      <td>-0.00005</td>\n",
       "      <td>-0.00005</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>33.33333</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>-0.00906</td>\n",
       "      <td>1.10444</td>\n",
       "      <td>1.10420</td>\n",
       "      <td>1.10397</td>\n",
       "      <td>42.81995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739166</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.10400</td>\n",
       "      <td>1.10400</td>\n",
       "      <td>1.1038</td>\n",
       "      <td>1.1039</td>\n",
       "      <td>1.10416</td>\n",
       "      <td>1.10412</td>\n",
       "      <td>1.10390</td>\n",
       "      <td>-0.00007</td>\n",
       "      <td>-0.00006</td>\n",
       "      <td>-0.00001</td>\n",
       "      <td>16.65011</td>\n",
       "      <td>13.88337</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>-0.00906</td>\n",
       "      <td>1.10445</td>\n",
       "      <td>1.10420</td>\n",
       "      <td>1.10394</td>\n",
       "      <td>40.40055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739167</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.10400</td>\n",
       "      <td>1.10410</td>\n",
       "      <td>1.1040</td>\n",
       "      <td>1.1041</td>\n",
       "      <td>1.10414</td>\n",
       "      <td>1.10412</td>\n",
       "      <td>1.10407</td>\n",
       "      <td>-0.00007</td>\n",
       "      <td>-0.00006</td>\n",
       "      <td>-0.00001</td>\n",
       "      <td>49.99007</td>\n",
       "      <td>22.21339</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.01812</td>\n",
       "      <td>1.10445</td>\n",
       "      <td>1.10420</td>\n",
       "      <td>1.10394</td>\n",
       "      <td>46.86666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739168</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.10400</td>\n",
       "      <td>1.10400</td>\n",
       "      <td>1.1040</td>\n",
       "      <td>1.1040</td>\n",
       "      <td>1.10413</td>\n",
       "      <td>1.10410</td>\n",
       "      <td>1.10400</td>\n",
       "      <td>-0.00008</td>\n",
       "      <td>-0.00006</td>\n",
       "      <td>-0.00001</td>\n",
       "      <td>33.32009</td>\n",
       "      <td>33.32009</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>-0.00906</td>\n",
       "      <td>1.10445</td>\n",
       "      <td>1.10418</td>\n",
       "      <td>1.10391</td>\n",
       "      <td>44.27986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739169</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.10400</td>\n",
       "      <td>1.10400</td>\n",
       "      <td>1.1038</td>\n",
       "      <td>1.1039</td>\n",
       "      <td>1.10411</td>\n",
       "      <td>1.10406</td>\n",
       "      <td>1.10390</td>\n",
       "      <td>-0.00009</td>\n",
       "      <td>-0.00007</td>\n",
       "      <td>-0.00002</td>\n",
       "      <td>16.65011</td>\n",
       "      <td>33.32009</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>-0.00906</td>\n",
       "      <td>1.10446</td>\n",
       "      <td>1.10416</td>\n",
       "      <td>1.10387</td>\n",
       "      <td>41.79552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739170</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.10390</td>\n",
       "      <td>1.10420</td>\n",
       "      <td>1.1039</td>\n",
       "      <td>1.1041</td>\n",
       "      <td>1.10408</td>\n",
       "      <td>1.10407</td>\n",
       "      <td>1.10407</td>\n",
       "      <td>-0.00008</td>\n",
       "      <td>-0.00007</td>\n",
       "      <td>-0.00001</td>\n",
       "      <td>49.99007</td>\n",
       "      <td>33.32009</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.01812</td>\n",
       "      <td>1.10446</td>\n",
       "      <td>1.10416</td>\n",
       "      <td>1.10387</td>\n",
       "      <td>48.07078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Target_1  Target_3  Target_5  Target_10  Target_15  Target_30  \\\n",
       "739156         0         0         0          0          0          0   \n",
       "739157         0         1         0          0          0          0   \n",
       "739158         0         1         1          0          0          0   \n",
       "739159         1         0         0          0          0          0   \n",
       "739160         0         0         0          0          0          0   \n",
       "739161         0         0         0          0          0          0   \n",
       "739162         1         0         0          0          0          0   \n",
       "739163         0         0         0          0          0          0   \n",
       "739164         0         0         0          0          0          0   \n",
       "739165         0         0         1          0          0          0   \n",
       "739166         1         0         0          0          0          0   \n",
       "739167         0         0         0          0          0          0   \n",
       "739168         0         0         0          0          0          0   \n",
       "739169         1         0         0          0          0          0   \n",
       "739170         0         0         0          0          0          0   \n",
       "\n",
       "           open     high     low   close      sma      ema     vwap     macd  \\\n",
       "739156  1.10430  1.10440  1.1042  1.1044  1.10424  1.10424  1.10433 -0.00006   \n",
       "739157  1.10440  1.10440  1.1042  1.1043  1.10425  1.10425  1.10430 -0.00005   \n",
       "739158  1.10420  1.10420  1.1040  1.1041  1.10423  1.10422  1.10410 -0.00006   \n",
       "739159  1.10415  1.10415  1.1040  1.1041  1.10422  1.10420  1.10408 -0.00006   \n",
       "739160  1.10430  1.10440  1.1042  1.1044  1.10425  1.10424  1.10433 -0.00004   \n",
       "739161  1.10440  1.10440  1.1043  1.1043  1.10425  1.10425  1.10433 -0.00003   \n",
       "739162  1.10420  1.10430  1.1041  1.1041  1.10424  1.10422  1.10417 -0.00004   \n",
       "739163  1.10420  1.10430  1.1042  1.1043  1.10425  1.10423  1.10427 -0.00003   \n",
       "739164  1.10420  1.10420  1.1040  1.1041  1.10423  1.10421  1.10410 -0.00004   \n",
       "739165  1.10410  1.10410  1.1040  1.1040  1.10421  1.10417  1.10403 -0.00005   \n",
       "739166  1.10400  1.10400  1.1038  1.1039  1.10416  1.10412  1.10390 -0.00007   \n",
       "739167  1.10400  1.10410  1.1040  1.1041  1.10414  1.10412  1.10407 -0.00007   \n",
       "739168  1.10400  1.10400  1.1040  1.1040  1.10413  1.10410  1.10400 -0.00008   \n",
       "739169  1.10400  1.10400  1.1038  1.1039  1.10411  1.10406  1.10390 -0.00009   \n",
       "739170  1.10390  1.10420  1.1039  1.1041  1.10408  1.10407  1.10407 -0.00008   \n",
       "\n",
       "        macd_signal  macd_hist     fast_k    fast_d     mom      roc  \\\n",
       "739156     -0.00012    0.00006  100.00000  80.00000  0.0002  0.01812   \n",
       "739157     -0.00010    0.00005   80.00000  80.00000 -0.0001 -0.00906   \n",
       "739158     -0.00009    0.00004   40.00000  73.33333 -0.0002 -0.01811   \n",
       "739159     -0.00009    0.00002   40.00000  53.33333  0.0000  0.00000   \n",
       "739160     -0.00008    0.00004  100.00000  60.00000  0.0003  0.02718   \n",
       "739161     -0.00007    0.00004   75.00000  71.66667 -0.0001 -0.00906   \n",
       "739162     -0.00006    0.00002   25.00000  66.66667 -0.0002 -0.01811   \n",
       "739163     -0.00006    0.00003   75.00000  58.33333  0.0002  0.01812   \n",
       "739164     -0.00005    0.00001   25.00000  41.66667 -0.0002 -0.01811   \n",
       "739165     -0.00005   -0.00000    0.00000  33.33333 -0.0001 -0.00906   \n",
       "739166     -0.00006   -0.00001   16.65011  13.88337 -0.0001 -0.00906   \n",
       "739167     -0.00006   -0.00001   49.99007  22.21339  0.0002  0.01812   \n",
       "739168     -0.00006   -0.00001   33.32009  33.32009 -0.0001 -0.00906   \n",
       "739169     -0.00007   -0.00002   16.65011  33.32009 -0.0001 -0.00906   \n",
       "739170     -0.00007   -0.00001   49.99007  33.32009  0.0002  0.01812   \n",
       "\n",
       "        upper_band  middle_band  lower_band       rsi  \n",
       "739156     1.10440      1.10415     1.10390  53.91597  \n",
       "739157     1.10441      1.10415     1.10389  50.09284  \n",
       "739158     1.10441      1.10415     1.10389  43.45594  \n",
       "739159     1.10441      1.10416     1.10390  43.45594  \n",
       "739160     1.10444      1.10417     1.10390  54.04749  \n",
       "739161     1.10446      1.10418     1.10390  50.64224  \n",
       "739162     1.10446      1.10418     1.10390  44.59111  \n",
       "739163     1.10447      1.10419     1.10390  50.90820  \n",
       "739164     1.10445      1.10420     1.10394  45.34127  \n",
       "739165     1.10444      1.10420     1.10397  42.81995  \n",
       "739166     1.10445      1.10420     1.10394  40.40055  \n",
       "739167     1.10445      1.10420     1.10394  46.86666  \n",
       "739168     1.10445      1.10418     1.10391  44.27986  \n",
       "739169     1.10446      1.10416     1.10387  41.79552  \n",
       "739170     1.10446      1.10416     1.10387  48.07078  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the last instances of the new data dataframe\n",
    "data.tail(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04059b2-5bba-4233-8250-4d11f243130a",
   "metadata": {},
   "source": [
    "## 4. Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf1b0df",
   "metadata": {},
   "source": [
    "Several machine learning models will be trained and tested for precision across different time intervals. The following performance metrics will be printed for each factor combination of train size, model, time interval, and value:\n",
    "- Precision\n",
    "- Recall\n",
    "- F1-Score\n",
    "\n",
    "The machine learning models used will be:\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f625627b",
   "metadata": {},
   "source": [
    "### 4.1 Initial testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22433842",
   "metadata": {},
   "source": [
    "An initial testing of the models with default parameters will be done to analyse different training sizes and intervals used. A test size of 2000 will be used to test the results and obtain the performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b03d3949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_report(value: int, value_str: str, train_size: int, model_name: str, target: str, report: dict) -> list:\n",
    "    \"\"\"\n",
    "    Generates a single row of classification metrics for a given class label.\n",
    "\n",
    "    This function extracts precision, recall, F1-score, and support from the classification report dictionary.\n",
    "\n",
    "    Args:\n",
    "        value (int): The numeric class label.\n",
    "        value_str (str): The string version of the class label used as a key in the classification report dictionary.\n",
    "        train_size (int): The size of the training set used to for the model.\n",
    "        model_name (str): A string identifier for the model used.\n",
    "        target (str): The target variable name.\n",
    "        report (dict): A dictionary produced by `classification_report` containing metrics for each class label.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of metrics for each factor combination.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create each row according to each combination of factors\n",
    "    report_row = [\n",
    "        train_size,\n",
    "        model_name,\n",
    "        target,\n",
    "        value,\n",
    "        report[value_str][\"precision\"],\n",
    "        report[value_str][\"recall\"],\n",
    "        report[value_str][\"f1-score\"],\n",
    "        report[value_str][\"support\"]\n",
    "    ]\n",
    "\n",
    "    return report_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5186855a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(train: pd.DataFrame, test: pd.DataFrame, model: object, model_name: str, train_size: int, Targets: list, predictors: list) -> list:\n",
    "    \"\"\"\n",
    "    Fits a given model on a training set, evaluates it on a test set for multiple targets, and returns a list of classification metrics for each target and class label.\n",
    "\n",
    "    Args:\n",
    "        train (pd.DataFrame): The training portion of the dataset, containing predictor and target columns.\n",
    "        test (pd.DataFrame): The testing portion of the dataset to evaluate the model's performance.\n",
    "        model (object): An instantiated ML model.\n",
    "        model_name (str): Identifier for the model.\n",
    "        train_size (int): Number of rows used for training.\n",
    "        Targets (list): A list of target column names.\n",
    "        predictors (list): A list of column names in `train` and `test` used as features.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of rows, where each row contains:\n",
    "    \"\"\"\n",
    "\n",
    "    # List with the results for the metrics of the model\n",
    "    df_results = []\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(train[predictors], train[Targets])\n",
    "\n",
    "    # Predict\n",
    "    y_pred = model.predict(test[predictors])\n",
    "    y_pred = pd.DataFrame(y_pred, columns=Targets)\n",
    "    \n",
    "    # Generate the classification report and metrics for each target variable.\n",
    "    for target in Targets:\n",
    "        report = classification_report(\n",
    "            y_true = test[target],\n",
    "            y_pred = y_pred[target],\n",
    "            zero_division = 0,\n",
    "            output_dict = True\n",
    "        )\n",
    "        \n",
    "        # Value 0\n",
    "        df_results.append(create_report(0, \"0\", train_size, model_name, target, report))\n",
    "\n",
    "        # Value 1\n",
    "        df_results.append(create_report(1, \"1\", train_size, model_name, target, report))\n",
    "    \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b681024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_models(data: pd.DataFrame, predictors: list, Targets: list, methods: dict, train_sizes: list, test_size: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluates multiple models on different train sizes, returning a combined DataFrame of classification metrics for each train size, model, and target.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The full dataset including both predictor columns and the target columns.\n",
    "        predictors (list): Column names in `data` to be used as features during model training and prediction.\n",
    "        Targets (list): Column names in `data` to be used as target variables.\n",
    "        methods (dict): A dictionary containign the models to evaluate.\n",
    "        train_sizes (list): A list of integers specifying the number of rows to use for training \n",
    "        test_size (int): Number of rows from the end of the dataset to reserve for testing in each iteration.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A concatenated multi-level indexed DataFrame containing classification metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    # DataFrame with the results from all training sizes\n",
    "    df_results_all = []\n",
    "\n",
    "    # Run the model for different train sizes\n",
    "    for train_size in train_sizes:\n",
    "\n",
    "        # Status update\n",
    "        print(\"Currently training a train size of:\", str(train_size))\n",
    "\n",
    "        # Create train and test subsets\n",
    "        train = data.iloc[-(train_size + test_size): -test_size]  # the last train_size instances before testing\n",
    "        test = data.iloc[-test_size:]     # last test_size rows instances for testing\n",
    "\n",
    "        # List for this train size metric results\n",
    "        df_results_loop = []\n",
    "        \n",
    "        # Evaluate the metrics for every model for this train size\n",
    "        for model_name, model in methods.items():\n",
    "            # Status update\n",
    "            print(\"Currently training the model:\", str(model_name))\n",
    "            t0 = time.time()\n",
    "\n",
    "            df_results_loop.extend(evaluate_model(train, test, model, model_name, train_size, Targets, predictors))\n",
    "\n",
    "            elapsed = time.time() - t0\n",
    "            print(f\"Time = {elapsed:.2f}s\")\n",
    "\n",
    "        # Convert results to a DataFrame\n",
    "        df_results_loop = pd.DataFrame(df_results_loop, columns = [\"Train_Size\", \"Model\", \"Variable\", \"Value\", \"Precision\", \"Recall\", \"F1-Score\", \"Support\"])\n",
    "        \n",
    "        # Set multi-level index\n",
    "        df_results_loop.set_index(\n",
    "            [\"Train_Size\", \"Model\", \"Variable\", \"Value\"],\n",
    "            inplace=True\n",
    "        )\n",
    "\n",
    "        df_results_all.append(df_results_loop)\n",
    "\n",
    "    # Concatenate all results\n",
    "    return pd.concat(df_results_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0065c742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently training a train size of: 100000\n",
      "Currently training the model: Decision Tree Classifier\n",
      "Time = 4.38s\n",
      "Currently training the model: Random Forest Classifier\n",
      "Time = 14.61s\n",
      "Currently training the model: XGBoost\n",
      "Time = 2.91s\n",
      "Currently training a train size of: 50000\n",
      "Currently training the model: Decision Tree Classifier\n",
      "Time = 1.93s\n",
      "Currently training the model: Random Forest Classifier\n",
      "Time = 6.58s\n",
      "Currently training the model: XGBoost\n",
      "Time = 1.69s\n",
      "Currently training a train size of: 25000\n",
      "Currently training the model: Decision Tree Classifier\n",
      "Time = 0.81s\n",
      "Currently training the model: Random Forest Classifier\n",
      "Time = 2.88s\n",
      "Currently training the model: XGBoost\n",
      "Time = 1.32s\n",
      "Currently training a train size of: 10000\n",
      "Currently training the model: Decision Tree Classifier\n",
      "Time = 0.31s\n",
      "Currently training the model: Random Forest Classifier\n",
      "Time = 1.18s\n",
      "Currently training the model: XGBoost\n",
      "Time = 1.06s\n",
      "Currently training a train size of: 5000\n",
      "Currently training the model: Decision Tree Classifier\n",
      "Time = 0.19s\n",
      "Currently training the model: Random Forest Classifier\n",
      "Time = 0.70s\n",
      "Currently training the model: XGBoost\n",
      "Time = 0.96s\n"
     ]
    }
   ],
   "source": [
    "# Dictionary of models to test\n",
    "methods = {\n",
    "    'Decision Tree Classifier': DecisionTreeClassifier(random_state = 77),\n",
    "    'Random Forest Classifier': RandomForestClassifier(random_state = 77, n_jobs = -1),\n",
    "    'XGBoost': xgb.XGBClassifier(random_state = 77, n_jobs = -1),\n",
    "}\n",
    "\n",
    "# Train and test sizes to test\n",
    "train_sizes = [100000, 50000, 25000, 10000, 5000]\n",
    "test_size = 2000\n",
    "\n",
    "metric_results = test_models(data, predictors, Targets, methods, train_sizes, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60fea2b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Support</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train_Size</th>\n",
       "      <th>Model</th>\n",
       "      <th>Variable</th>\n",
       "      <th>Value</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">100000</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">Decision Tree Classifier</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">Target_1</th>\n",
       "      <th>0</th>\n",
       "      <td>0.629848</td>\n",
       "      <td>0.609299</td>\n",
       "      <td>0.619403</td>\n",
       "      <td>1226.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.411548</td>\n",
       "      <td>0.432817</td>\n",
       "      <td>0.421914</td>\n",
       "      <td>774.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Target_3</th>\n",
       "      <th>0</th>\n",
       "      <td>0.602927</td>\n",
       "      <td>0.519328</td>\n",
       "      <td>0.558014</td>\n",
       "      <td>1190.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.413333</td>\n",
       "      <td>0.497531</td>\n",
       "      <td>0.451541</td>\n",
       "      <td>810.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Target_5</th>\n",
       "      <th>0</th>\n",
       "      <td>0.587049</td>\n",
       "      <td>0.474678</td>\n",
       "      <td>0.524917</td>\n",
       "      <td>1165.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">5000</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">XGBoost</th>\n",
       "      <th>Target_10</th>\n",
       "      <th>1</th>\n",
       "      <td>0.468966</td>\n",
       "      <td>0.699429</td>\n",
       "      <td>0.561468</td>\n",
       "      <td>875.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Target_15</th>\n",
       "      <th>0</th>\n",
       "      <td>0.576238</td>\n",
       "      <td>0.271709</td>\n",
       "      <td>0.369289</td>\n",
       "      <td>1071.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.769645</td>\n",
       "      <td>0.589934</td>\n",
       "      <td>929.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Target_30</th>\n",
       "      <th>0</th>\n",
       "      <td>0.630824</td>\n",
       "      <td>0.166825</td>\n",
       "      <td>0.263868</td>\n",
       "      <td>1055.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.489250</td>\n",
       "      <td>0.891005</td>\n",
       "      <td>0.631658</td>\n",
       "      <td>945.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Precision    Recall  \\\n",
       "Train_Size Model                    Variable  Value                        \n",
       "100000     Decision Tree Classifier Target_1  0       0.629848  0.609299   \n",
       "                                              1       0.411548  0.432817   \n",
       "                                    Target_3  0       0.602927  0.519328   \n",
       "                                              1       0.413333  0.497531   \n",
       "                                    Target_5  0       0.587049  0.474678   \n",
       "...                                                        ...       ...   \n",
       "5000       XGBoost                  Target_10 1       0.468966  0.699429   \n",
       "                                    Target_15 0       0.576238  0.271709   \n",
       "                                              1       0.478261  0.769645   \n",
       "                                    Target_30 0       0.630824  0.166825   \n",
       "                                              1       0.489250  0.891005   \n",
       "\n",
       "                                                     F1-Score  Support  \n",
       "Train_Size Model                    Variable  Value                     \n",
       "100000     Decision Tree Classifier Target_1  0      0.619403   1226.0  \n",
       "                                              1      0.421914    774.0  \n",
       "                                    Target_3  0      0.558014   1190.0  \n",
       "                                              1      0.451541    810.0  \n",
       "                                    Target_5  0      0.524917   1165.0  \n",
       "...                                                       ...      ...  \n",
       "5000       XGBoost                  Target_10 1      0.561468    875.0  \n",
       "                                    Target_15 0      0.369289   1071.0  \n",
       "                                              1      0.589934    929.0  \n",
       "                                    Target_30 0      0.263868   1055.0  \n",
       "                                              1      0.631658    945.0  \n",
       "\n",
       "[180 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the some instances of the metrics dataframe\n",
    "metric_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28dcdf9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Support</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train_Size</th>\n",
       "      <th>Model</th>\n",
       "      <th>Variable</th>\n",
       "      <th>Value</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50000</th>\n",
       "      <th>Random Forest Classifier</th>\n",
       "      <th>Target_1</th>\n",
       "      <th>0</th>\n",
       "      <td>0.634547</td>\n",
       "      <td>0.725122</td>\n",
       "      <td>0.676818</td>\n",
       "      <td>1226.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">100000</th>\n",
       "      <th>Random Forest Classifier</th>\n",
       "      <th>Target_1</th>\n",
       "      <th>0</th>\n",
       "      <td>0.625348</td>\n",
       "      <td>0.732463</td>\n",
       "      <td>0.674681</td>\n",
       "      <td>1226.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <th>Target_1</th>\n",
       "      <th>0</th>\n",
       "      <td>0.627874</td>\n",
       "      <td>0.712887</td>\n",
       "      <td>0.667685</td>\n",
       "      <td>1226.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <th>Random Forest Classifier</th>\n",
       "      <th>Target_1</th>\n",
       "      <th>0</th>\n",
       "      <td>0.634559</td>\n",
       "      <td>0.703915</td>\n",
       "      <td>0.667440</td>\n",
       "      <td>1226.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <th>Random Forest Classifier</th>\n",
       "      <th>Target_1</th>\n",
       "      <th>0</th>\n",
       "      <td>0.627941</td>\n",
       "      <td>0.696574</td>\n",
       "      <td>0.660480</td>\n",
       "      <td>1226.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">25000</th>\n",
       "      <th>Random Forest Classifier</th>\n",
       "      <th>Target_1</th>\n",
       "      <th>0</th>\n",
       "      <td>0.634572</td>\n",
       "      <td>0.682708</td>\n",
       "      <td>0.657760</td>\n",
       "      <td>1226.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <th>Target_1</th>\n",
       "      <th>0</th>\n",
       "      <td>0.642072</td>\n",
       "      <td>0.667210</td>\n",
       "      <td>0.654400</td>\n",
       "      <td>1226.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50000</th>\n",
       "      <th>XGBoost</th>\n",
       "      <th>Target_1</th>\n",
       "      <th>0</th>\n",
       "      <td>0.641066</td>\n",
       "      <td>0.667210</td>\n",
       "      <td>0.653877</td>\n",
       "      <td>1226.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <th>XGBoost</th>\n",
       "      <th>Target_1</th>\n",
       "      <th>0</th>\n",
       "      <td>0.642324</td>\n",
       "      <td>0.631321</td>\n",
       "      <td>0.636775</td>\n",
       "      <td>1226.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <th>XGBoost</th>\n",
       "      <th>Target_1</th>\n",
       "      <th>0</th>\n",
       "      <td>0.636594</td>\n",
       "      <td>0.615824</td>\n",
       "      <td>0.626036</td>\n",
       "      <td>1226.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100000</th>\n",
       "      <th>Decision Tree Classifier</th>\n",
       "      <th>Target_1</th>\n",
       "      <th>0</th>\n",
       "      <td>0.629848</td>\n",
       "      <td>0.609299</td>\n",
       "      <td>0.619403</td>\n",
       "      <td>1226.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50000</th>\n",
       "      <th>Decision Tree Classifier</th>\n",
       "      <th>Target_1</th>\n",
       "      <th>0</th>\n",
       "      <td>0.621598</td>\n",
       "      <td>0.577488</td>\n",
       "      <td>0.598732</td>\n",
       "      <td>1226.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25000</th>\n",
       "      <th>Decision Tree Classifier</th>\n",
       "      <th>Target_1</th>\n",
       "      <th>0</th>\n",
       "      <td>0.613460</td>\n",
       "      <td>0.579935</td>\n",
       "      <td>0.596226</td>\n",
       "      <td>1226.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <th>Decision Tree Classifier</th>\n",
       "      <th>Target_1</th>\n",
       "      <th>0</th>\n",
       "      <td>0.625117</td>\n",
       "      <td>0.544046</td>\n",
       "      <td>0.581771</td>\n",
       "      <td>1226.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <th>Random Forest Classifier</th>\n",
       "      <th>Target_3</th>\n",
       "      <th>0</th>\n",
       "      <td>0.612426</td>\n",
       "      <td>0.521849</td>\n",
       "      <td>0.563521</td>\n",
       "      <td>1190.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50000</th>\n",
       "      <th>Random Forest Classifier</th>\n",
       "      <th>Target_3</th>\n",
       "      <th>0</th>\n",
       "      <td>0.607004</td>\n",
       "      <td>0.524370</td>\n",
       "      <td>0.562669</td>\n",
       "      <td>1190.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">100000</th>\n",
       "      <th>Random Forest Classifier</th>\n",
       "      <th>Target_3</th>\n",
       "      <th>0</th>\n",
       "      <td>0.600191</td>\n",
       "      <td>0.528571</td>\n",
       "      <td>0.562109</td>\n",
       "      <td>1190.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <th>Target_3</th>\n",
       "      <th>0</th>\n",
       "      <td>0.626819</td>\n",
       "      <td>0.506723</td>\n",
       "      <td>0.560409</td>\n",
       "      <td>1190.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree Classifier</th>\n",
       "      <th>Target_3</th>\n",
       "      <th>0</th>\n",
       "      <td>0.602927</td>\n",
       "      <td>0.519328</td>\n",
       "      <td>0.558014</td>\n",
       "      <td>1190.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <th>Random Forest Classifier</th>\n",
       "      <th>Target_3</th>\n",
       "      <th>0</th>\n",
       "      <td>0.611504</td>\n",
       "      <td>0.509244</td>\n",
       "      <td>0.555708</td>\n",
       "      <td>1190.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Precision    Recall  \\\n",
       "Train_Size Model                    Variable Value                        \n",
       "50000      Random Forest Classifier Target_1 0       0.634547  0.725122   \n",
       "100000     Random Forest Classifier Target_1 0       0.625348  0.732463   \n",
       "           XGBoost                  Target_1 0       0.627874  0.712887   \n",
       "5000       Random Forest Classifier Target_1 0       0.634559  0.703915   \n",
       "10000      Random Forest Classifier Target_1 0       0.627941  0.696574   \n",
       "25000      Random Forest Classifier Target_1 0       0.634572  0.682708   \n",
       "           XGBoost                  Target_1 0       0.642072  0.667210   \n",
       "50000      XGBoost                  Target_1 0       0.641066  0.667210   \n",
       "10000      XGBoost                  Target_1 0       0.642324  0.631321   \n",
       "5000       XGBoost                  Target_1 0       0.636594  0.615824   \n",
       "100000     Decision Tree Classifier Target_1 0       0.629848  0.609299   \n",
       "50000      Decision Tree Classifier Target_1 0       0.621598  0.577488   \n",
       "25000      Decision Tree Classifier Target_1 0       0.613460  0.579935   \n",
       "10000      Decision Tree Classifier Target_1 0       0.625117  0.544046   \n",
       "5000       Random Forest Classifier Target_3 0       0.612426  0.521849   \n",
       "50000      Random Forest Classifier Target_3 0       0.607004  0.524370   \n",
       "100000     Random Forest Classifier Target_3 0       0.600191  0.528571   \n",
       "           XGBoost                  Target_3 0       0.626819  0.506723   \n",
       "           Decision Tree Classifier Target_3 0       0.602927  0.519328   \n",
       "10000      Random Forest Classifier Target_3 0       0.611504  0.509244   \n",
       "\n",
       "                                                    F1-Score  Support  \n",
       "Train_Size Model                    Variable Value                     \n",
       "50000      Random Forest Classifier Target_1 0      0.676818   1226.0  \n",
       "100000     Random Forest Classifier Target_1 0      0.674681   1226.0  \n",
       "           XGBoost                  Target_1 0      0.667685   1226.0  \n",
       "5000       Random Forest Classifier Target_1 0      0.667440   1226.0  \n",
       "10000      Random Forest Classifier Target_1 0      0.660480   1226.0  \n",
       "25000      Random Forest Classifier Target_1 0      0.657760   1226.0  \n",
       "           XGBoost                  Target_1 0      0.654400   1226.0  \n",
       "50000      XGBoost                  Target_1 0      0.653877   1226.0  \n",
       "10000      XGBoost                  Target_1 0      0.636775   1226.0  \n",
       "5000       XGBoost                  Target_1 0      0.626036   1226.0  \n",
       "100000     Decision Tree Classifier Target_1 0      0.619403   1226.0  \n",
       "50000      Decision Tree Classifier Target_1 0      0.598732   1226.0  \n",
       "25000      Decision Tree Classifier Target_1 0      0.596226   1226.0  \n",
       "10000      Decision Tree Classifier Target_1 0      0.581771   1226.0  \n",
       "5000       Random Forest Classifier Target_3 0      0.563521   1190.0  \n",
       "50000      Random Forest Classifier Target_3 0      0.562669   1190.0  \n",
       "100000     Random Forest Classifier Target_3 0      0.562109   1190.0  \n",
       "           XGBoost                  Target_3 0      0.560409   1190.0  \n",
       "           Decision Tree Classifier Target_3 0      0.558014   1190.0  \n",
       "10000      Random Forest Classifier Target_3 0      0.555708   1190.0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the best performing models from the DataFrame\n",
    "filtered_metrics = metric_results[\n",
    "    (metric_results[\"Precision\"] >= 0.5) &\n",
    "    (metric_results[\"Recall\"] >= 0.5) &\n",
    "    (metric_results[\"F1-Score\"] >= 0.5)\n",
    "]\n",
    "\n",
    "filtered_metrics.sort_values(by=\"F1-Score\", ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ed4e56",
   "metadata": {},
   "source": [
    "From the metrics observed a model can be selected according to the best performing one in this phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567ae11f",
   "metadata": {},
   "source": [
    "### 4.2 Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73968fc",
   "metadata": {},
   "source": [
    "The model selected for further fine tuning will be the model shown as the best in the metrics DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cff182f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Support</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train_Size</th>\n",
       "      <th>Model</th>\n",
       "      <th>Variable</th>\n",
       "      <th>Value</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50000</th>\n",
       "      <th>Random Forest Classifier</th>\n",
       "      <th>Target_1</th>\n",
       "      <th>0</th>\n",
       "      <td>0.634547</td>\n",
       "      <td>0.725122</td>\n",
       "      <td>0.676818</td>\n",
       "      <td>1226.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Precision    Recall  \\\n",
       "Train_Size Model                    Variable Value                        \n",
       "50000      Random Forest Classifier Target_1 0       0.634547  0.725122   \n",
       "\n",
       "                                                    F1-Score  Support  \n",
       "Train_Size Model                    Variable Value                     \n",
       "50000      Random Forest Classifier Target_1 0      0.676818   1226.0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best model\n",
    "filtered_metrics.sort_values(by=\"F1-Score\", ascending=False).head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da8a2a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the training characteristics for the best initial model\n",
    "initial_parameters = filtered_metrics[\"F1-Score\"].idxmax()\n",
    "\n",
    "train_size, model_name, target, label = initial_parameters\n",
    "model = methods[initial_parameters[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e54aef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the new train and test DataFrames\n",
    "train = data.iloc[-(train_size + test_size): -test_size]\n",
    "test = data.iloc[-test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80b98c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.70      0.66      1226\n",
      "           1       0.43      0.36      0.39       774\n",
      "\n",
      "    accuracy                           0.57      2000\n",
      "   macro avg       0.53      0.53      0.53      2000\n",
      "weighted avg       0.56      0.57      0.56      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analyse performance before fine tuning\n",
    "model.fit(train[predictors], train[target])\n",
    "y_pred = model.predict(test[predictors])\n",
    "\n",
    "print(classification_report(y_pred = y_pred, y_true = test[target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e5735d",
   "metadata": {},
   "source": [
    "### 4.3 Fine Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f29b72d",
   "metadata": {},
   "source": [
    "The hyperparameters for the model selected will be fine tuned using Bayesian Optimization. The objective will be to optimize for the label that had the best performance in the model.\n",
    "\n",
    "To avoid creating an overly biased model, the average F1-score across both labels will be the score to maximize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af0bd4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified scorer for f1-score\n",
    "modified_f1_scorer = make_scorer(\n",
    "    f1_score,\n",
    "    average = \"macro\",\n",
    "    zero_division = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c13e2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_name: str, params: dict) -> object:\n",
    "    \"\"\"\n",
    "    Creates and returns a model instance with valid parameters.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The model type.\n",
    "        params (dict): Dictionary of hyperparameters.\n",
    "\n",
    "    Returns:\n",
    "        object: A machine learning model instance.\n",
    "    \"\"\"\n",
    "\n",
    "    # Selects the corresponding model\n",
    "    if model_name == \"XGBoost\":\n",
    "        model = xgb.XGBClassifier(**params, use_label_encoder = False)\n",
    "    \n",
    "    elif model_name == \"Random Forest Classifier\":\n",
    "        model = RandomForestClassifier(**params)\n",
    "    \n",
    "    elif model_name == \"Decision Tree Classifier\":\n",
    "        model = DecisionTreeClassifier(**params)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a26b98cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial, model_name: str, train_size: int, test_size: int, data: pd.DataFrame, predictors: list, target: str, modified_scorer: object) -> float:\n",
    "    \"\"\"\n",
    "    Objective function for Optuna hyperparameter optimization.\n",
    "\n",
    "    Args:\n",
    "        trial (optuna.Trial): An Optuna trial object used to sample hyperparameters.\n",
    "        model_name (str): The model type.\n",
    "        train_size (int): Maximum number of rows for training in TimeSeriesSplit.\n",
    "        test_size (int): Number of rows for testing in TimeSeriesSplit.\n",
    "        data (pd.DataFrame): The full dataset containing both features and the target variable.\n",
    "        predictors (list): Names of the columns in `data` used as input features.\n",
    "        target (str): The name of the target column in `data` for the supervised learning task.\n",
    "        modified_scorer (object): A scikit-learn compatible scoring function or scorer object used for evaluating model performance in cross-validation.\n",
    "\n",
    "    Returns:\n",
    "        float: The mean cross-validation score.\n",
    "    \"\"\"\n",
    "\n",
    "    # Select the corresponding model\n",
    "    if model_name == \"XGBoost\":\n",
    "\n",
    "        # XGBoost-specific search space\n",
    "        param_grid = {\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),                             # Maximum depth of each tree\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 1000),                    # Number of trees or boosting rounds\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 0.3, log = True),    # Step size for boosting updates\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.4, 1.0),                        # Fraction of data per boosting iteration\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),          # Fraction of features per tree\n",
    "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-6, 10.0, log = True),            # L1 regularization\n",
    "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-3, 5.0, log = True),           # L2 regularization\n",
    "            \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 20),               # Minimum sum of instance weight for further split\n",
    "            \"gamma\": trial.suggest_float(\"gamma\", 0, 5),                                    # Minimum loss reduction for further split\n",
    "            \n",
    "            # Fixed parameters\n",
    "            \"objective\": \"binary:logistic\",     # XGBoost binary classification\n",
    "            \"n_jobs\": -1,                       # Use all available cores\n",
    "            \"random_state\": 77                  # Ensures reproducibility\n",
    "        }\n",
    "        \n",
    "    elif model_name == \"Random Forest Classifier\":\n",
    "\n",
    "        # RandomForestClassifier-specific search space\n",
    "        param_grid = {\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),                                 # Maximum depth of each tree\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 1000),                        # Number of trees or boosting rounds\n",
    "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 10),                 # Minimum samples per split\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10),                   # Minimum samples per leaf\n",
    "            \"max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None]),  # Number of features to consider when looking for the best split\n",
    "            \"criterion\": trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"]),           # Measure used to split nodes\n",
    "\n",
    "            # Fixed parameters\n",
    "            \"n_jobs\": -1,           # Use all available cores\n",
    "            \"random_state\": 77      # Ensures reproducibility\n",
    "        }\n",
    "        \n",
    "    elif model_name == \"Decision Tree Classifier\":\n",
    "\n",
    "        # DecisionTreeClassifier-specific search space\n",
    "        param_grid = {\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),                         # Maximum depth of each tree\n",
    "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 10),         # Minimum samples per split\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10),           # Minimum samples per leaf\n",
    "            \"splitter\": trial.suggest_categorical(\"splitter\", [\"best\", \"random\"]),      # Defines the splitting strategy\n",
    "            \"criterion\": trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"]),   # Measure used to split nodes\n",
    "            \n",
    "            # Fixed parameters\n",
    "            \"random_state\": 77      # Ensures reproducibility\n",
    "        }\n",
    "\n",
    "    # Create the model instance with the corresponding parameters\n",
    "    model = create_model(model_name, param_grid)\n",
    "\n",
    "    # Time Series Cross-Validation\n",
    "    tscv = TimeSeriesSplit(n_splits = 5, max_train_size = train_size, test_size = test_size)\n",
    "\n",
    "    # Create scoring metric\n",
    "    score = cross_val_score(model, data[predictors], data[target], cv = tscv, scoring = modified_scorer).mean()\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b4e8c45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-25 22:37:54,415] A new study created in memory with name: model_optimization\n"
     ]
    }
   ],
   "source": [
    "# Create the study for the Bayesian Optimization, maximizing f1-score\n",
    "study = optuna.create_study(\n",
    "    direction = \"maximize\",\n",
    "    sampler = optuna.samplers.TPESampler(n_startup_trials = 15),\n",
    "    pruner = optuna.pruners.MedianPruner(n_warmup_steps = 15),\n",
    "    study_name = \"model_optimization\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "96e46368",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-25 22:41:06,844] Trial 0 finished with value: 0.403176246643637 and parameters: {'max_depth': 3, 'n_estimators': 92, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 'log2', 'criterion': 'entropy'}. Best is trial 0 with value: 0.403176246643637.\n",
      "[I 2025-04-25 22:43:01,072] Trial 9 finished with value: 0.43247545979743596 and parameters: {'max_depth': 4, 'n_estimators': 338, 'min_samples_split': 2, 'min_samples_leaf': 6, 'max_features': 'sqrt', 'criterion': 'entropy'}. Best is trial 9 with value: 0.43247545979743596.\n",
      "[I 2025-04-25 22:43:51,261] Trial 5 finished with value: 0.47613680736796715 and parameters: {'max_depth': 10, 'n_estimators': 223, 'min_samples_split': 7, 'min_samples_leaf': 2, 'max_features': 'log2', 'criterion': 'entropy'}. Best is trial 5 with value: 0.47613680736796715.\n",
      "[I 2025-04-25 22:43:52,601] Trial 2 finished with value: 0.49598726975167795 and parameters: {'max_depth': 12, 'n_estimators': 85, 'min_samples_split': 3, 'min_samples_leaf': 10, 'max_features': None, 'criterion': 'gini'}. Best is trial 2 with value: 0.49598726975167795.\n",
      "[I 2025-04-25 22:44:29,379] Trial 7 finished with value: 0.4665569439920553 and parameters: {'max_depth': 8, 'n_estimators': 339, 'min_samples_split': 9, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'criterion': 'entropy'}. Best is trial 2 with value: 0.49598726975167795.\n",
      "[I 2025-04-25 22:45:34,458] Trial 11 finished with value: 0.47020770725685246 and parameters: {'max_depth': 8, 'n_estimators': 577, 'min_samples_split': 8, 'min_samples_leaf': 6, 'max_features': 'log2', 'criterion': 'gini'}. Best is trial 2 with value: 0.49598726975167795.\n",
      "[I 2025-04-25 22:47:14,295] Trial 12 finished with value: 0.5029525649669011 and parameters: {'max_depth': 14, 'n_estimators': 279, 'min_samples_split': 3, 'min_samples_leaf': 8, 'max_features': 'sqrt', 'criterion': 'gini'}. Best is trial 12 with value: 0.5029525649669011.\n",
      "[I 2025-04-25 22:47:16,088] Trial 10 finished with value: 0.4690456993958804 and parameters: {'max_depth': 3, 'n_estimators': 500, 'min_samples_split': 3, 'min_samples_leaf': 4, 'max_features': None, 'criterion': 'entropy'}. Best is trial 12 with value: 0.5029525649669011.\n",
      "[I 2025-04-25 22:47:17,366] Trial 1 finished with value: 0.4677658663195025 and parameters: {'max_depth': 6, 'n_estimators': 295, 'min_samples_split': 10, 'min_samples_leaf': 3, 'max_features': None, 'criterion': 'gini'}. Best is trial 12 with value: 0.5029525649669011.\n",
      "[I 2025-04-25 22:48:09,409] Trial 13 finished with value: 0.48381905261667846 and parameters: {'max_depth': 10, 'n_estimators': 55, 'min_samples_split': 6, 'min_samples_leaf': 5, 'max_features': None, 'criterion': 'entropy'}. Best is trial 12 with value: 0.5029525649669011.\n",
      "[I 2025-04-25 22:48:38,417] Trial 16 finished with value: 0.5087600910638498 and parameters: {'max_depth': 15, 'n_estimators': 55, 'min_samples_split': 3, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'criterion': 'entropy'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-25 22:48:43,740] Trial 15 finished with value: 0.49549296071479876 and parameters: {'max_depth': 13, 'n_estimators': 148, 'min_samples_split': 4, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-25 22:49:18,236] Trial 3 finished with value: 0.45285342240636073 and parameters: {'max_depth': 5, 'n_estimators': 424, 'min_samples_split': 5, 'min_samples_leaf': 9, 'max_features': None, 'criterion': 'entropy'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-25 22:50:05,527] Trial 14 finished with value: 0.4986464897456694 and parameters: {'max_depth': 14, 'n_estimators': 245, 'min_samples_split': 4, 'min_samples_leaf': 5, 'max_features': 'log2', 'criterion': 'entropy'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-25 22:50:47,781] Trial 17 finished with value: 0.4364136948275024 and parameters: {'max_depth': 5, 'n_estimators': 298, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 'log2', 'criterion': 'entropy'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-25 22:51:51,166] Trial 8 finished with value: 0.48145702106422006 and parameters: {'max_depth': 10, 'n_estimators': 895, 'min_samples_split': 2, 'min_samples_leaf': 9, 'max_features': 'log2', 'criterion': 'entropy'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-25 22:53:40,209] Trial 4 finished with value: 0.4575323623098395 and parameters: {'max_depth': 3, 'n_estimators': 960, 'min_samples_split': 4, 'min_samples_leaf': 2, 'max_features': None, 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-25 22:53:58,385] Trial 18 finished with value: 0.4766315827272967 and parameters: {'max_depth': 8, 'n_estimators': 370, 'min_samples_split': 8, 'min_samples_leaf': 9, 'max_features': 'log2', 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-25 22:54:03,273] Trial 6 finished with value: 0.48023683362806163 and parameters: {'max_depth': 9, 'n_estimators': 433, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None, 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-25 22:56:01,132] Trial 24 finished with value: 0.43543433429492107 and parameters: {'max_depth': 4, 'n_estimators': 410, 'min_samples_split': 3, 'min_samples_leaf': 8, 'max_features': 'log2', 'criterion': 'entropy'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-25 22:58:02,237] Trial 21 finished with value: 0.47145800987812825 and parameters: {'max_depth': 8, 'n_estimators': 562, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2', 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-25 22:58:26,702] Trial 22 finished with value: 0.4764548871458444 and parameters: {'max_depth': 10, 'n_estimators': 452, 'min_samples_split': 4, 'min_samples_leaf': 6, 'max_features': 'log2', 'criterion': 'entropy'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-25 22:58:50,926] Trial 20 finished with value: 0.4512635800550421 and parameters: {'max_depth': 6, 'n_estimators': 847, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2', 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-25 23:05:04,654] Trial 32 finished with value: 0.5005276600553937 and parameters: {'max_depth': 15, 'n_estimators': 196, 'min_samples_split': 4, 'min_samples_leaf': 7, 'max_features': 'sqrt', 'criterion': 'entropy'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-25 23:05:31,993] Trial 33 finished with value: 0.4993575239953838 and parameters: {'max_depth': 15, 'n_estimators': 198, 'min_samples_split': 5, 'min_samples_leaf': 7, 'max_features': 'sqrt', 'criterion': 'entropy'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-25 23:05:48,268] Trial 34 finished with value: 0.5007947817672775 and parameters: {'max_depth': 15, 'n_estimators': 197, 'min_samples_split': 5, 'min_samples_leaf': 7, 'max_features': 'sqrt', 'criterion': 'entropy'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-25 23:05:55,369] Trial 19 finished with value: 0.49554668568687604 and parameters: {'max_depth': 13, 'n_estimators': 364, 'min_samples_split': 7, 'min_samples_leaf': 4, 'max_features': None, 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-25 23:08:37,304] Trial 29 finished with value: 0.5018114411947832 and parameters: {'max_depth': 15, 'n_estimators': 612, 'min_samples_split': 5, 'min_samples_leaf': 7, 'max_features': 'sqrt', 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-25 23:09:03,072] Trial 27 finished with value: 0.501976170796424 and parameters: {'max_depth': 15, 'n_estimators': 725, 'min_samples_split': 5, 'min_samples_leaf': 7, 'max_features': 'sqrt', 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-25 23:09:28,135] Trial 30 finished with value: 0.5013822927802719 and parameters: {'max_depth': 15, 'n_estimators': 641, 'min_samples_split': 5, 'min_samples_leaf': 7, 'max_features': 'sqrt', 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-25 23:09:45,123] Trial 23 finished with value: 0.47702420435028986 and parameters: {'max_depth': 8, 'n_estimators': 572, 'min_samples_split': 3, 'min_samples_leaf': 9, 'max_features': None, 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-25 23:10:03,452] Trial 26 finished with value: 0.5036225296487964 and parameters: {'max_depth': 15, 'n_estimators': 876, 'min_samples_split': 5, 'min_samples_leaf': 7, 'max_features': 'sqrt', 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-25 23:10:34,961] Trial 31 finished with value: 0.5010616254253825 and parameters: {'max_depth': 15, 'n_estimators': 644, 'min_samples_split': 5, 'min_samples_leaf': 7, 'max_features': 'sqrt', 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-25 23:11:07,006] Trial 35 finished with value: 0.5014954506714455 and parameters: {'max_depth': 15, 'n_estimators': 185, 'min_samples_split': 5, 'min_samples_leaf': 8, 'max_features': 'sqrt', 'criterion': 'entropy'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-25 23:11:12,075] Trial 36 finished with value: 0.501343602544739 and parameters: {'max_depth': 15, 'n_estimators': 158, 'min_samples_split': 3, 'min_samples_leaf': 7, 'max_features': 'sqrt', 'criterion': 'entropy'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-25 23:11:27,734] Trial 28 finished with value: 0.5032174807016674 and parameters: {'max_depth': 15, 'n_estimators': 743, 'min_samples_split': 5, 'min_samples_leaf': 7, 'max_features': 'sqrt', 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-25 23:19:03,390] Trial 25 finished with value: 0.4912963075817207 and parameters: {'max_depth': 12, 'n_estimators': 697, 'min_samples_split': 3, 'min_samples_leaf': 4, 'max_features': None, 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-25 23:20:48,527] Trial 37 finished with value: 0.49087388359535505 and parameters: {'max_depth': 12, 'n_estimators': 682, 'min_samples_split': 6, 'min_samples_leaf': 8, 'max_features': 'sqrt', 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-25 23:23:08,146] Trial 38 finished with value: 0.49103899926244254 and parameters: {'max_depth': 12, 'n_estimators': 713, 'min_samples_split': 6, 'min_samples_leaf': 8, 'max_features': 'sqrt', 'criterion': 'entropy'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-25 23:23:21,794] Trial 39 finished with value: 0.4908652138869927 and parameters: {'max_depth': 12, 'n_estimators': 675, 'min_samples_split': 3, 'min_samples_leaf': 8, 'max_features': 'sqrt', 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-25 23:23:35,024] Trial 40 finished with value: 0.49087017166651653 and parameters: {'max_depth': 12, 'n_estimators': 680, 'min_samples_split': 6, 'min_samples_leaf': 8, 'max_features': 'sqrt', 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-25 23:24:25,823] Trial 41 finished with value: 0.49446468999660664 and parameters: {'max_depth': 12, 'n_estimators': 720, 'min_samples_split': 6, 'min_samples_leaf': 10, 'max_features': 'sqrt', 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-25 23:25:08,361] Trial 42 finished with value: 0.49661870780183054 and parameters: {'max_depth': 13, 'n_estimators': 697, 'min_samples_split': 6, 'min_samples_leaf': 8, 'max_features': 'sqrt', 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-25 23:26:26,533] Trial 43 finished with value: 0.49527385888707337 and parameters: {'max_depth': 13, 'n_estimators': 721, 'min_samples_split': 6, 'min_samples_leaf': 8, 'max_features': 'sqrt', 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-25 23:26:44,984] Trial 44 finished with value: 0.49475675699145727 and parameters: {'max_depth': 13, 'n_estimators': 737, 'min_samples_split': 6, 'min_samples_leaf': 8, 'max_features': 'sqrt', 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-25 23:27:09,233] Trial 45 finished with value: 0.4954629008181312 and parameters: {'max_depth': 13, 'n_estimators': 748, 'min_samples_split': 6, 'min_samples_leaf': 8, 'max_features': 'sqrt', 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-25 23:28:13,808] Trial 46 finished with value: 0.4943034511312199 and parameters: {'max_depth': 13, 'n_estimators': 782, 'min_samples_split': 6, 'min_samples_leaf': 8, 'max_features': 'sqrt', 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-25 23:28:22,407] Trial 47 finished with value: 0.4919082450286535 and parameters: {'max_depth': 12, 'n_estimators': 796, 'min_samples_split': 6, 'min_samples_leaf': 8, 'max_features': 'sqrt', 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-25 23:36:10,125] Trial 48 finished with value: 0.4932381952085222 and parameters: {'max_depth': 13, 'n_estimators': 802, 'min_samples_split': 6, 'min_samples_leaf': 8, 'max_features': 'sqrt', 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-25 23:37:22,858] Trial 49 finished with value: 0.4947815670993944 and parameters: {'max_depth': 13, 'n_estimators': 778, 'min_samples_split': 6, 'min_samples_leaf': 8, 'max_features': 'sqrt', 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-25 23:40:22,134] Trial 50 finished with value: 0.5004518470022982 and parameters: {'max_depth': 14, 'n_estimators': 791, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'sqrt', 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-25 23:40:36,492] Trial 52 finished with value: 0.5008520167276116 and parameters: {'max_depth': 14, 'n_estimators': 769, 'min_samples_split': 4, 'min_samples_leaf': 10, 'max_features': 'sqrt', 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-25 23:40:47,186] Trial 51 finished with value: 0.4995556591233988 and parameters: {'max_depth': 14, 'n_estimators': 804, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'sqrt', 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-25 23:41:23,849] Trial 53 finished with value: 0.49984690569861934 and parameters: {'max_depth': 14, 'n_estimators': 807, 'min_samples_split': 4, 'min_samples_leaf': 6, 'max_features': 'sqrt', 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-25 23:41:31,002] Trial 54 finished with value: 0.5001503060723173 and parameters: {'max_depth': 14, 'n_estimators': 777, 'min_samples_split': 4, 'min_samples_leaf': 6, 'max_features': 'sqrt', 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-25 23:44:06,846] Trial 55 finished with value: 0.4995375844996687 and parameters: {'max_depth': 14, 'n_estimators': 783, 'min_samples_split': 10, 'min_samples_leaf': 6, 'max_features': 'sqrt', 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-25 23:44:37,291] Trial 56 finished with value: 0.4999669376931884 and parameters: {'max_depth': 14, 'n_estimators': 801, 'min_samples_split': 4, 'min_samples_leaf': 6, 'max_features': 'sqrt', 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-25 23:44:55,106] Trial 57 finished with value: 0.5001472804988968 and parameters: {'max_depth': 14, 'n_estimators': 803, 'min_samples_split': 4, 'min_samples_leaf': 6, 'max_features': 'sqrt', 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-25 23:50:07,672] Trial 58 finished with value: 0.49871374732267865 and parameters: {'max_depth': 14, 'n_estimators': 961, 'min_samples_split': 4, 'min_samples_leaf': 6, 'max_features': 'sqrt', 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-25 23:50:11,269] Trial 59 finished with value: 0.498956967869574 and parameters: {'max_depth': 14, 'n_estimators': 965, 'min_samples_split': 4, 'min_samples_leaf': 6, 'max_features': 'sqrt', 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-26 00:00:06,120] Trial 60 finished with value: 0.4990211502341323 and parameters: {'max_depth': 14, 'n_estimators': 982, 'min_samples_split': 10, 'min_samples_leaf': 6, 'max_features': 'sqrt', 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-26 00:01:01,783] Trial 61 finished with value: 0.4986968056463471 and parameters: {'max_depth': 14, 'n_estimators': 999, 'min_samples_split': 4, 'min_samples_leaf': 6, 'max_features': 'sqrt', 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-26 00:03:34,335] Trial 63 finished with value: 0.4993259794937627 and parameters: {'max_depth': 14, 'n_estimators': 912, 'min_samples_split': 4, 'min_samples_leaf': 6, 'max_features': 'sqrt', 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-26 00:04:44,035] Trial 62 finished with value: 0.49871374732267865 and parameters: {'max_depth': 14, 'n_estimators': 961, 'min_samples_split': 4, 'min_samples_leaf': 6, 'max_features': 'sqrt', 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-26 00:05:02,100] Trial 64 finished with value: 0.4993259794937627 and parameters: {'max_depth': 14, 'n_estimators': 912, 'min_samples_split': 4, 'min_samples_leaf': 6, 'max_features': 'sqrt', 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-26 00:06:04,071] Trial 66 finished with value: 0.49865895471352967 and parameters: {'max_depth': 14, 'n_estimators': 963, 'min_samples_split': 7, 'min_samples_leaf': 5, 'max_features': 'sqrt', 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-26 00:06:38,517] Trial 65 finished with value: 0.49678968677137203 and parameters: {'max_depth': 14, 'n_estimators': 997, 'min_samples_split': 7, 'min_samples_leaf': 5, 'max_features': 'sqrt', 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-26 00:08:16,514] Trial 67 finished with value: 0.501297748410843 and parameters: {'max_depth': 15, 'n_estimators': 955, 'min_samples_split': 7, 'min_samples_leaf': 9, 'max_features': 'sqrt', 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-26 00:09:44,894] Trial 69 finished with value: 0.5001910376347785 and parameters: {'max_depth': 15, 'n_estimators': 925, 'min_samples_split': 7, 'min_samples_leaf': 5, 'max_features': 'sqrt', 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-26 00:11:04,139] Trial 73 finished with value: 0.49953155771171315 and parameters: {'max_depth': 15, 'n_estimators': 85, 'min_samples_split': 5, 'min_samples_leaf': 9, 'max_features': None, 'criterion': 'entropy'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-26 00:13:58,584] Trial 74 finished with value: 0.4994997646386191 and parameters: {'max_depth': 15, 'n_estimators': 83, 'min_samples_split': 5, 'min_samples_leaf': 9, 'max_features': None, 'criterion': 'entropy'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-26 00:15:38,768] Trial 75 finished with value: 0.4996166386352761 and parameters: {'max_depth': 15, 'n_estimators': 86, 'min_samples_split': 5, 'min_samples_leaf': 9, 'max_features': None, 'criterion': 'entropy'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-26 00:17:16,861] Trial 76 finished with value: 0.499536354600293 and parameters: {'max_depth': 15, 'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 9, 'max_features': None, 'criterion': 'entropy'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-26 00:17:31,088] Trial 77 finished with value: 0.4999531382365793 and parameters: {'max_depth': 15, 'n_estimators': 89, 'min_samples_split': 5, 'min_samples_leaf': 9, 'max_features': None, 'criterion': 'entropy'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-26 00:19:35,066] Trial 78 finished with value: 0.49945275157095087 and parameters: {'max_depth': 15, 'n_estimators': 110, 'min_samples_split': 5, 'min_samples_leaf': 9, 'max_features': None, 'criterion': 'entropy'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-26 00:21:09,158] Trial 79 finished with value: 0.4916487957750647 and parameters: {'max_depth': 15, 'n_estimators': 113, 'min_samples_split': 5, 'min_samples_leaf': 7, 'max_features': None, 'criterion': 'entropy'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-26 00:23:07,314] Trial 80 finished with value: 0.49455574973260025 and parameters: {'max_depth': 15, 'n_estimators': 109, 'min_samples_split': 5, 'min_samples_leaf': 3, 'max_features': None, 'criterion': 'entropy'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-26 00:25:42,410] Trial 72 finished with value: 0.49827552491254695 and parameters: {'max_depth': 15, 'n_estimators': 914, 'min_samples_split': 5, 'min_samples_leaf': 9, 'max_features': 'sqrt', 'criterion': 'entropy'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-26 00:26:06,602] Trial 81 finished with value: 0.49150291022685905 and parameters: {'max_depth': 15, 'n_estimators': 130, 'min_samples_split': 5, 'min_samples_leaf': 7, 'max_features': None, 'criterion': 'entropy'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-26 00:28:28,940] Trial 68 finished with value: 0.48921010525083003 and parameters: {'max_depth': 11, 'n_estimators': 967, 'min_samples_split': 7, 'min_samples_leaf': 5, 'max_features': None, 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-26 00:31:32,560] Trial 82 finished with value: 0.49920792310068907 and parameters: {'max_depth': 15, 'n_estimators': 597, 'min_samples_split': 5, 'min_samples_leaf': 7, 'max_features': 'log2', 'criterion': 'entropy'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-26 00:33:25,869] Trial 85 finished with value: 0.4968382408694949 and parameters: {'max_depth': 15, 'n_estimators': 525, 'min_samples_split': 2, 'min_samples_leaf': 7, 'max_features': 'log2', 'criterion': 'entropy'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-26 00:33:33,267] Trial 83 finished with value: 0.4981081916210434 and parameters: {'max_depth': 15, 'n_estimators': 627, 'min_samples_split': 5, 'min_samples_leaf': 7, 'max_features': 'log2', 'criterion': 'entropy'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-26 00:33:42,367] Trial 86 finished with value: 0.4721568058051341 and parameters: {'max_depth': 9, 'n_estimators': 646, 'min_samples_split': 3, 'min_samples_leaf': 7, 'max_features': 'log2', 'criterion': 'entropy'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-26 00:35:04,043] Trial 84 finished with value: 0.4971543234807846 and parameters: {'max_depth': 15, 'n_estimators': 642, 'min_samples_split': 5, 'min_samples_leaf': 7, 'max_features': 'log2', 'criterion': 'entropy'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-26 00:36:05,198] Trial 87 finished with value: 0.48337360268066243 and parameters: {'max_depth': 11, 'n_estimators': 625, 'min_samples_split': 2, 'min_samples_leaf': 7, 'max_features': 'log2', 'criterion': 'entropy'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-26 00:37:16,156] Trial 88 finished with value: 0.4841785241820086 and parameters: {'max_depth': 11, 'n_estimators': 620, 'min_samples_split': 2, 'min_samples_leaf': 7, 'max_features': 'log2', 'criterion': 'entropy'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-26 00:37:33,327] Trial 92 finished with value: 0.4745796189684744 and parameters: {'max_depth': 9, 'n_estimators': 246, 'min_samples_split': 3, 'min_samples_leaf': 7, 'max_features': 'sqrt', 'criterion': 'entropy'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-26 00:37:44,462] Trial 90 finished with value: 0.4632149566510456 and parameters: {'max_depth': 7, 'n_estimators': 637, 'min_samples_split': 2, 'min_samples_leaf': 7, 'max_features': 'sqrt', 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-26 00:38:06,715] Trial 89 finished with value: 0.48601376321799367 and parameters: {'max_depth': 11, 'n_estimators': 642, 'min_samples_split': 2, 'min_samples_leaf': 7, 'max_features': 'log2', 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-26 00:38:11,262] Trial 94 finished with value: 0.4619297067523216 and parameters: {'max_depth': 7, 'n_estimators': 162, 'min_samples_split': 3, 'min_samples_leaf': 7, 'max_features': 'sqrt', 'criterion': 'entropy'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-26 00:38:13,201] Trial 95 finished with value: 0.4627384213451163 and parameters: {'max_depth': 7, 'n_estimators': 169, 'min_samples_split': 3, 'min_samples_leaf': 7, 'max_features': 'sqrt', 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-26 00:39:22,003] Trial 91 finished with value: 0.47926399785748053 and parameters: {'max_depth': 9, 'n_estimators': 628, 'min_samples_split': 2, 'min_samples_leaf': 7, 'max_features': 'log2', 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-26 00:39:33,988] Trial 93 finished with value: 0.4987102033560918 and parameters: {'max_depth': 15, 'n_estimators': 276, 'min_samples_split': 3, 'min_samples_leaf': 7, 'max_features': 'sqrt', 'criterion': 'entropy'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-26 00:39:35,059] Trial 70 finished with value: 0.5006680063970499 and parameters: {'max_depth': 15, 'n_estimators': 896, 'min_samples_split': 7, 'min_samples_leaf': 5, 'max_features': None, 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-26 00:39:35,981] Trial 71 finished with value: 0.4846251545994861 and parameters: {'max_depth': 11, 'n_estimators': 911, 'min_samples_split': 7, 'min_samples_leaf': 9, 'max_features': None, 'criterion': 'entropy'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-26 00:39:45,686] Trial 97 finished with value: 0.46113030227257834 and parameters: {'max_depth': 7, 'n_estimators': 251, 'min_samples_split': 3, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-26 00:39:49,322] Trial 96 finished with value: 0.5004648036669017 and parameters: {'max_depth': 15, 'n_estimators': 273, 'min_samples_split': 3, 'min_samples_leaf': 7, 'max_features': 'sqrt', 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-26 00:40:11,433] Trial 98 finished with value: 0.4995312478426103 and parameters: {'max_depth': 15, 'n_estimators': 276, 'min_samples_split': 3, 'min_samples_leaf': 7, 'max_features': 'sqrt', 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n",
      "[I 2025-04-26 00:40:25,940] Trial 99 finished with value: 0.45189301140837823 and parameters: {'max_depth': 6, 'n_estimators': 836, 'min_samples_split': 3, 'min_samples_leaf': 3, 'max_features': 'sqrt', 'criterion': 'gini'}. Best is trial 16 with value: 0.5087600910638498.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'max_depth': 15, 'n_estimators': 55, 'min_samples_split': 3, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'criterion': 'entropy'}\n",
      "Best F1-Score: 0.5087600910638498\n"
     ]
    }
   ],
   "source": [
    "# Run the Bayesian Optimization\n",
    "study.optimize(lambda trial: objective(trial, model_name, train_size, test_size, data, predictors, target, modified_f1_scorer), n_trials = 100, n_jobs = -1)\n",
    "\n",
    "#Best parameters\n",
    "print(\"Best Params:\", study.best_trial.params)\n",
    "print(\"Best F1-Score:\", study.best_trial.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a697a7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.79      0.70      1226\n",
      "           1       0.44      0.25      0.32       774\n",
      "\n",
      "    accuracy                           0.58      2000\n",
      "   macro avg       0.53      0.52      0.51      2000\n",
      "weighted avg       0.55      0.58      0.55      2000\n",
      "\n",
      "{0: 1550, 1: 450}\n"
     ]
    }
   ],
   "source": [
    "# Create the model with the best parameters\n",
    "model.set_params(**study.best_params)\n",
    "model.fit(train[predictors], train[target])\n",
    "y_pred = model.predict(test[predictors])\n",
    "\n",
    "# Analyse performance after fine tuning\n",
    "print(classification_report(y_pred = y_pred, y_true = test[target]))\n",
    "unique, counts = np.unique(y_pred, return_counts = True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97996792",
   "metadata": {},
   "source": [
    "The optimization of the hyperparameters shows a good performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f895be",
   "metadata": {},
   "source": [
    "### 4.4 Backtesting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9507742c",
   "metadata": {},
   "source": [
    "After obtaining the best model from the Bayesian Optimization, backtesting will be done across the dataset to ensure performance stays consistent across different conditions.\n",
    "\n",
    "A backtesting algorithm will be constructed by training over a set number of instances and testing the next determined number of rows and rolling over. Therefore, training and testing over a rolling window over the whole dataset will be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9687c1d4-1be0-47f4-9fa7-cf8452c74c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtesting(data: pd.DataFrame, model: object, predictors: list, target: str, train_size: int, test_size: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform a rolling window backtesting procedure on a time series dataset.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The full dataset containing both features and the target variable.\n",
    "        model (object): A scikit-learn compatible estimator that implements the `fit` and `predict` methods.\n",
    "        predictors (list): Names of the columns in `data` that serve as input features for the model.\n",
    "        target (str): The name of the column in `data` representing the target variable to predict.\n",
    "        train_size (int): Number of rows to use as the rolling training set in each iteration.\n",
    "        test_size (int): Number of rows to use as the test set in each iteration.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A concatenated DataFrame containing the actual and predicted values for each test window.\n",
    "    \"\"\"\n",
    "\n",
    "    # DataFrame for the final predictions\n",
    "    predictions = []\n",
    "    n_loops = 0\n",
    "\n",
    "    # The maximum index for starting the rolling window\n",
    "    max_index = data.shape[0] - train_size - test_size\n",
    "\n",
    "    # Calculate how many loops will be run\n",
    "    total_loops = (max_index // test_size) + 1\n",
    "\n",
    "    print(f\"Starting backtesting. Will run {total_loops} loops in total.\")\n",
    "\n",
    "    # Run each rolling window of training and testing\n",
    "    for i in range(0, max_index + 1, test_size):\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Rolling train window\n",
    "        train = data.iloc[i : i + train_size]\n",
    "\n",
    "        # Next 'test_size' rows for testing\n",
    "        test = data.iloc[i + train_size : i + train_size + test_size]\n",
    "\n",
    "        # DataFrame to store predictions for this iteration\n",
    "        preds_chunk = pd.DataFrame(index = test.index)\n",
    "\n",
    "        # Train on the rolling window\n",
    "        model.fit(train[predictors], train[target])\n",
    "        y_pred = model.predict(test[predictors])\n",
    "\n",
    "        # Store actual and predicted columns\n",
    "        preds_chunk[target] = test[target]\n",
    "        preds_chunk[f\"Predicted_{target}\"] = y_pred\n",
    "\n",
    "        predictions.append(preds_chunk)\n",
    "\n",
    "        n_loops += 1\n",
    "        elapsed = time.time() - t0\n",
    "        print(\n",
    "            f\"Loop {n_loops} of {total_loops}: \"\n",
    "            f\"train = [{i}:{i + train_size}], \"\n",
    "            f\"test = [{i + train_size}:{i + train_size + test_size}], \"\n",
    "            f\"time = {elapsed:.2f}s\"\n",
    "        )\n",
    "\n",
    "    # Concatenate the results from all test windows\n",
    "    return pd.concat(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6a5ee41b-cc06-45fd-af4a-e7fbfda0bd47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting backtesting. Will run 344 loops in total.\n",
      "Loop 1 of 344: train = [0:50000], test = [50000:52000], time = 1.62s\n",
      "Loop 2 of 344: train = [2000:52000], test = [52000:54000], time = 1.64s\n",
      "Loop 3 of 344: train = [4000:54000], test = [54000:56000], time = 1.62s\n",
      "Loop 4 of 344: train = [6000:56000], test = [56000:58000], time = 1.66s\n",
      "Loop 5 of 344: train = [8000:58000], test = [58000:60000], time = 1.68s\n",
      "Loop 6 of 344: train = [10000:60000], test = [60000:62000], time = 1.64s\n",
      "Loop 7 of 344: train = [12000:62000], test = [62000:64000], time = 1.69s\n",
      "Loop 8 of 344: train = [14000:64000], test = [64000:66000], time = 1.70s\n",
      "Loop 9 of 344: train = [16000:66000], test = [66000:68000], time = 1.75s\n",
      "Loop 10 of 344: train = [18000:68000], test = [68000:70000], time = 1.77s\n",
      "Loop 11 of 344: train = [20000:70000], test = [70000:72000], time = 1.77s\n",
      "Loop 12 of 344: train = [22000:72000], test = [72000:74000], time = 1.81s\n",
      "Loop 13 of 344: train = [24000:74000], test = [74000:76000], time = 1.80s\n",
      "Loop 14 of 344: train = [26000:76000], test = [76000:78000], time = 1.83s\n",
      "Loop 15 of 344: train = [28000:78000], test = [78000:80000], time = 1.82s\n",
      "Loop 16 of 344: train = [30000:80000], test = [80000:82000], time = 1.85s\n",
      "Loop 17 of 344: train = [32000:82000], test = [82000:84000], time = 1.81s\n",
      "Loop 18 of 344: train = [34000:84000], test = [84000:86000], time = 1.81s\n",
      "Loop 19 of 344: train = [36000:86000], test = [86000:88000], time = 1.82s\n",
      "Loop 20 of 344: train = [38000:88000], test = [88000:90000], time = 1.85s\n",
      "Loop 21 of 344: train = [40000:90000], test = [90000:92000], time = 1.84s\n",
      "Loop 22 of 344: train = [42000:92000], test = [92000:94000], time = 1.79s\n",
      "Loop 23 of 344: train = [44000:94000], test = [94000:96000], time = 1.82s\n",
      "Loop 24 of 344: train = [46000:96000], test = [96000:98000], time = 1.81s\n",
      "Loop 25 of 344: train = [48000:98000], test = [98000:100000], time = 1.88s\n",
      "Loop 26 of 344: train = [50000:100000], test = [100000:102000], time = 1.78s\n",
      "Loop 27 of 344: train = [52000:102000], test = [102000:104000], time = 1.77s\n",
      "Loop 28 of 344: train = [54000:104000], test = [104000:106000], time = 1.70s\n",
      "Loop 29 of 344: train = [56000:106000], test = [106000:108000], time = 1.74s\n",
      "Loop 30 of 344: train = [58000:108000], test = [108000:110000], time = 1.70s\n",
      "Loop 31 of 344: train = [60000:110000], test = [110000:112000], time = 1.67s\n",
      "Loop 32 of 344: train = [62000:112000], test = [112000:114000], time = 1.74s\n",
      "Loop 33 of 344: train = [64000:114000], test = [114000:116000], time = 1.81s\n",
      "Loop 34 of 344: train = [66000:116000], test = [116000:118000], time = 1.67s\n",
      "Loop 35 of 344: train = [68000:118000], test = [118000:120000], time = 1.72s\n",
      "Loop 36 of 344: train = [70000:120000], test = [120000:122000], time = 1.77s\n",
      "Loop 37 of 344: train = [72000:122000], test = [122000:124000], time = 1.79s\n",
      "Loop 38 of 344: train = [74000:124000], test = [124000:126000], time = 1.81s\n",
      "Loop 39 of 344: train = [76000:126000], test = [126000:128000], time = 1.78s\n",
      "Loop 40 of 344: train = [78000:128000], test = [128000:130000], time = 1.78s\n",
      "Loop 41 of 344: train = [80000:130000], test = [130000:132000], time = 1.79s\n",
      "Loop 42 of 344: train = [82000:132000], test = [132000:134000], time = 1.77s\n",
      "Loop 43 of 344: train = [84000:134000], test = [134000:136000], time = 1.75s\n",
      "Loop 44 of 344: train = [86000:136000], test = [136000:138000], time = 1.75s\n",
      "Loop 45 of 344: train = [88000:138000], test = [138000:140000], time = 1.78s\n",
      "Loop 46 of 344: train = [90000:140000], test = [140000:142000], time = 1.77s\n",
      "Loop 47 of 344: train = [92000:142000], test = [142000:144000], time = 1.71s\n",
      "Loop 48 of 344: train = [94000:144000], test = [144000:146000], time = 1.75s\n",
      "Loop 49 of 344: train = [96000:146000], test = [146000:148000], time = 1.71s\n",
      "Loop 50 of 344: train = [98000:148000], test = [148000:150000], time = 1.71s\n",
      "Loop 51 of 344: train = [100000:150000], test = [150000:152000], time = 1.73s\n",
      "Loop 52 of 344: train = [102000:152000], test = [152000:154000], time = 1.75s\n",
      "Loop 53 of 344: train = [104000:154000], test = [154000:156000], time = 1.69s\n",
      "Loop 54 of 344: train = [106000:156000], test = [156000:158000], time = 1.73s\n",
      "Loop 55 of 344: train = [108000:158000], test = [158000:160000], time = 1.69s\n",
      "Loop 56 of 344: train = [110000:160000], test = [160000:162000], time = 1.72s\n",
      "Loop 57 of 344: train = [112000:162000], test = [162000:164000], time = 1.72s\n",
      "Loop 58 of 344: train = [114000:164000], test = [164000:166000], time = 1.68s\n",
      "Loop 59 of 344: train = [116000:166000], test = [166000:168000], time = 1.64s\n",
      "Loop 60 of 344: train = [118000:168000], test = [168000:170000], time = 1.66s\n",
      "Loop 61 of 344: train = [120000:170000], test = [170000:172000], time = 1.64s\n",
      "Loop 62 of 344: train = [122000:172000], test = [172000:174000], time = 1.69s\n",
      "Loop 63 of 344: train = [124000:174000], test = [174000:176000], time = 1.73s\n",
      "Loop 64 of 344: train = [126000:176000], test = [176000:178000], time = 1.73s\n",
      "Loop 65 of 344: train = [128000:178000], test = [178000:180000], time = 1.78s\n",
      "Loop 66 of 344: train = [130000:180000], test = [180000:182000], time = 1.71s\n",
      "Loop 67 of 344: train = [132000:182000], test = [182000:184000], time = 1.76s\n",
      "Loop 68 of 344: train = [134000:184000], test = [184000:186000], time = 1.68s\n",
      "Loop 69 of 344: train = [136000:186000], test = [186000:188000], time = 1.69s\n",
      "Loop 70 of 344: train = [138000:188000], test = [188000:190000], time = 1.70s\n",
      "Loop 71 of 344: train = [140000:190000], test = [190000:192000], time = 1.70s\n",
      "Loop 72 of 344: train = [142000:192000], test = [192000:194000], time = 1.70s\n",
      "Loop 73 of 344: train = [144000:194000], test = [194000:196000], time = 1.79s\n",
      "Loop 74 of 344: train = [146000:196000], test = [196000:198000], time = 1.75s\n",
      "Loop 75 of 344: train = [148000:198000], test = [198000:200000], time = 1.78s\n",
      "Loop 76 of 344: train = [150000:200000], test = [200000:202000], time = 1.79s\n",
      "Loop 77 of 344: train = [152000:202000], test = [202000:204000], time = 1.77s\n",
      "Loop 78 of 344: train = [154000:204000], test = [204000:206000], time = 1.81s\n",
      "Loop 79 of 344: train = [156000:206000], test = [206000:208000], time = 1.85s\n",
      "Loop 80 of 344: train = [158000:208000], test = [208000:210000], time = 1.83s\n",
      "Loop 81 of 344: train = [160000:210000], test = [210000:212000], time = 1.84s\n",
      "Loop 82 of 344: train = [162000:212000], test = [212000:214000], time = 1.83s\n",
      "Loop 83 of 344: train = [164000:214000], test = [214000:216000], time = 1.80s\n",
      "Loop 84 of 344: train = [166000:216000], test = [216000:218000], time = 1.79s\n",
      "Loop 85 of 344: train = [168000:218000], test = [218000:220000], time = 1.80s\n",
      "Loop 86 of 344: train = [170000:220000], test = [220000:222000], time = 1.77s\n",
      "Loop 87 of 344: train = [172000:222000], test = [222000:224000], time = 1.77s\n",
      "Loop 88 of 344: train = [174000:224000], test = [224000:226000], time = 1.75s\n",
      "Loop 89 of 344: train = [176000:226000], test = [226000:228000], time = 1.77s\n",
      "Loop 90 of 344: train = [178000:228000], test = [228000:230000], time = 1.78s\n",
      "Loop 91 of 344: train = [180000:230000], test = [230000:232000], time = 1.73s\n",
      "Loop 92 of 344: train = [182000:232000], test = [232000:234000], time = 1.73s\n",
      "Loop 93 of 344: train = [184000:234000], test = [234000:236000], time = 1.66s\n",
      "Loop 94 of 344: train = [186000:236000], test = [236000:238000], time = 1.64s\n",
      "Loop 95 of 344: train = [188000:238000], test = [238000:240000], time = 1.65s\n",
      "Loop 96 of 344: train = [190000:240000], test = [240000:242000], time = 1.62s\n",
      "Loop 97 of 344: train = [192000:242000], test = [242000:244000], time = 1.75s\n",
      "Loop 98 of 344: train = [194000:244000], test = [244000:246000], time = 1.68s\n",
      "Loop 99 of 344: train = [196000:246000], test = [246000:248000], time = 1.75s\n",
      "Loop 100 of 344: train = [198000:248000], test = [248000:250000], time = 1.79s\n",
      "Loop 101 of 344: train = [200000:250000], test = [250000:252000], time = 1.76s\n",
      "Loop 102 of 344: train = [202000:252000], test = [252000:254000], time = 1.75s\n",
      "Loop 103 of 344: train = [204000:254000], test = [254000:256000], time = 1.73s\n",
      "Loop 104 of 344: train = [206000:256000], test = [256000:258000], time = 1.71s\n",
      "Loop 105 of 344: train = [208000:258000], test = [258000:260000], time = 1.74s\n",
      "Loop 106 of 344: train = [210000:260000], test = [260000:262000], time = 1.76s\n",
      "Loop 107 of 344: train = [212000:262000], test = [262000:264000], time = 1.77s\n",
      "Loop 108 of 344: train = [214000:264000], test = [264000:266000], time = 1.80s\n",
      "Loop 109 of 344: train = [216000:266000], test = [266000:268000], time = 1.72s\n",
      "Loop 110 of 344: train = [218000:268000], test = [268000:270000], time = 1.68s\n",
      "Loop 111 of 344: train = [220000:270000], test = [270000:272000], time = 1.69s\n",
      "Loop 112 of 344: train = [222000:272000], test = [272000:274000], time = 1.74s\n",
      "Loop 113 of 344: train = [224000:274000], test = [274000:276000], time = 1.78s\n",
      "Loop 114 of 344: train = [226000:276000], test = [276000:278000], time = 1.75s\n",
      "Loop 115 of 344: train = [228000:278000], test = [278000:280000], time = 1.73s\n",
      "Loop 116 of 344: train = [230000:280000], test = [280000:282000], time = 1.77s\n",
      "Loop 117 of 344: train = [232000:282000], test = [282000:284000], time = 1.82s\n",
      "Loop 118 of 344: train = [234000:284000], test = [284000:286000], time = 1.72s\n",
      "Loop 119 of 344: train = [236000:286000], test = [286000:288000], time = 1.74s\n",
      "Loop 120 of 344: train = [238000:288000], test = [288000:290000], time = 1.72s\n",
      "Loop 121 of 344: train = [240000:290000], test = [290000:292000], time = 1.78s\n",
      "Loop 122 of 344: train = [242000:292000], test = [292000:294000], time = 1.84s\n",
      "Loop 123 of 344: train = [244000:294000], test = [294000:296000], time = 1.82s\n",
      "Loop 124 of 344: train = [246000:296000], test = [296000:298000], time = 1.83s\n",
      "Loop 125 of 344: train = [248000:298000], test = [298000:300000], time = 1.80s\n",
      "Loop 126 of 344: train = [250000:300000], test = [300000:302000], time = 1.80s\n",
      "Loop 127 of 344: train = [252000:302000], test = [302000:304000], time = 1.81s\n",
      "Loop 128 of 344: train = [254000:304000], test = [304000:306000], time = 1.81s\n",
      "Loop 129 of 344: train = [256000:306000], test = [306000:308000], time = 1.88s\n",
      "Loop 130 of 344: train = [258000:308000], test = [308000:310000], time = 1.85s\n",
      "Loop 131 of 344: train = [260000:310000], test = [310000:312000], time = 1.84s\n",
      "Loop 132 of 344: train = [262000:312000], test = [312000:314000], time = 1.81s\n",
      "Loop 133 of 344: train = [264000:314000], test = [314000:316000], time = 1.80s\n",
      "Loop 134 of 344: train = [266000:316000], test = [316000:318000], time = 1.82s\n",
      "Loop 135 of 344: train = [268000:318000], test = [318000:320000], time = 1.80s\n",
      "Loop 136 of 344: train = [270000:320000], test = [320000:322000], time = 1.81s\n",
      "Loop 137 of 344: train = [272000:322000], test = [322000:324000], time = 1.87s\n",
      "Loop 138 of 344: train = [274000:324000], test = [324000:326000], time = 1.86s\n",
      "Loop 139 of 344: train = [276000:326000], test = [326000:328000], time = 1.86s\n",
      "Loop 140 of 344: train = [278000:328000], test = [328000:330000], time = 1.83s\n",
      "Loop 141 of 344: train = [280000:330000], test = [330000:332000], time = 1.86s\n",
      "Loop 142 of 344: train = [282000:332000], test = [332000:334000], time = 1.89s\n",
      "Loop 143 of 344: train = [284000:334000], test = [334000:336000], time = 1.89s\n",
      "Loop 144 of 344: train = [286000:336000], test = [336000:338000], time = 1.90s\n",
      "Loop 145 of 344: train = [288000:338000], test = [338000:340000], time = 1.97s\n",
      "Loop 146 of 344: train = [290000:340000], test = [340000:342000], time = 1.92s\n",
      "Loop 147 of 344: train = [292000:342000], test = [342000:344000], time = 1.88s\n",
      "Loop 148 of 344: train = [294000:344000], test = [344000:346000], time = 1.88s\n",
      "Loop 149 of 344: train = [296000:346000], test = [346000:348000], time = 1.89s\n",
      "Loop 150 of 344: train = [298000:348000], test = [348000:350000], time = 1.92s\n",
      "Loop 151 of 344: train = [300000:350000], test = [350000:352000], time = 1.88s\n",
      "Loop 152 of 344: train = [302000:352000], test = [352000:354000], time = 1.87s\n",
      "Loop 153 of 344: train = [304000:354000], test = [354000:356000], time = 1.92s\n",
      "Loop 154 of 344: train = [306000:356000], test = [356000:358000], time = 1.91s\n",
      "Loop 155 of 344: train = [308000:358000], test = [358000:360000], time = 1.88s\n",
      "Loop 156 of 344: train = [310000:360000], test = [360000:362000], time = 1.85s\n",
      "Loop 157 of 344: train = [312000:362000], test = [362000:364000], time = 1.83s\n",
      "Loop 158 of 344: train = [314000:364000], test = [364000:366000], time = 1.79s\n",
      "Loop 159 of 344: train = [316000:366000], test = [366000:368000], time = 1.77s\n",
      "Loop 160 of 344: train = [318000:368000], test = [368000:370000], time = 1.74s\n",
      "Loop 161 of 344: train = [320000:370000], test = [370000:372000], time = 1.73s\n",
      "Loop 162 of 344: train = [322000:372000], test = [372000:374000], time = 1.67s\n",
      "Loop 163 of 344: train = [324000:374000], test = [374000:376000], time = 1.72s\n",
      "Loop 164 of 344: train = [326000:376000], test = [376000:378000], time = 1.72s\n",
      "Loop 165 of 344: train = [328000:378000], test = [378000:380000], time = 1.75s\n",
      "Loop 166 of 344: train = [330000:380000], test = [380000:382000], time = 1.74s\n",
      "Loop 167 of 344: train = [332000:382000], test = [382000:384000], time = 1.68s\n",
      "Loop 168 of 344: train = [334000:384000], test = [384000:386000], time = 1.74s\n",
      "Loop 169 of 344: train = [336000:386000], test = [386000:388000], time = 1.71s\n",
      "Loop 170 of 344: train = [338000:388000], test = [388000:390000], time = 1.72s\n",
      "Loop 171 of 344: train = [340000:390000], test = [390000:392000], time = 1.67s\n",
      "Loop 172 of 344: train = [342000:392000], test = [392000:394000], time = 1.66s\n",
      "Loop 173 of 344: train = [344000:394000], test = [394000:396000], time = 1.67s\n",
      "Loop 174 of 344: train = [346000:396000], test = [396000:398000], time = 1.68s\n",
      "Loop 175 of 344: train = [348000:398000], test = [398000:400000], time = 1.70s\n",
      "Loop 176 of 344: train = [350000:400000], test = [400000:402000], time = 1.70s\n",
      "Loop 177 of 344: train = [352000:402000], test = [402000:404000], time = 1.69s\n",
      "Loop 178 of 344: train = [354000:404000], test = [404000:406000], time = 1.77s\n",
      "Loop 179 of 344: train = [356000:406000], test = [406000:408000], time = 1.75s\n",
      "Loop 180 of 344: train = [358000:408000], test = [408000:410000], time = 1.68s\n",
      "Loop 181 of 344: train = [360000:410000], test = [410000:412000], time = 1.72s\n",
      "Loop 182 of 344: train = [362000:412000], test = [412000:414000], time = 1.73s\n",
      "Loop 183 of 344: train = [364000:414000], test = [414000:416000], time = 1.74s\n",
      "Loop 184 of 344: train = [366000:416000], test = [416000:418000], time = 1.76s\n",
      "Loop 185 of 344: train = [368000:418000], test = [418000:420000], time = 1.73s\n",
      "Loop 186 of 344: train = [370000:420000], test = [420000:422000], time = 1.78s\n",
      "Loop 187 of 344: train = [372000:422000], test = [422000:424000], time = 1.71s\n",
      "Loop 188 of 344: train = [374000:424000], test = [424000:426000], time = 1.68s\n",
      "Loop 189 of 344: train = [376000:426000], test = [426000:428000], time = 1.74s\n",
      "Loop 190 of 344: train = [378000:428000], test = [428000:430000], time = 1.69s\n",
      "Loop 191 of 344: train = [380000:430000], test = [430000:432000], time = 1.73s\n",
      "Loop 192 of 344: train = [382000:432000], test = [432000:434000], time = 1.75s\n",
      "Loop 193 of 344: train = [384000:434000], test = [434000:436000], time = 1.71s\n",
      "Loop 194 of 344: train = [386000:436000], test = [436000:438000], time = 1.72s\n",
      "Loop 195 of 344: train = [388000:438000], test = [438000:440000], time = 1.71s\n",
      "Loop 196 of 344: train = [390000:440000], test = [440000:442000], time = 1.71s\n",
      "Loop 197 of 344: train = [392000:442000], test = [442000:444000], time = 1.68s\n",
      "Loop 198 of 344: train = [394000:444000], test = [444000:446000], time = 1.72s\n",
      "Loop 199 of 344: train = [396000:446000], test = [446000:448000], time = 1.74s\n",
      "Loop 200 of 344: train = [398000:448000], test = [448000:450000], time = 1.72s\n",
      "Loop 201 of 344: train = [400000:450000], test = [450000:452000], time = 1.71s\n",
      "Loop 202 of 344: train = [402000:452000], test = [452000:454000], time = 1.71s\n",
      "Loop 203 of 344: train = [404000:454000], test = [454000:456000], time = 1.65s\n",
      "Loop 204 of 344: train = [406000:456000], test = [456000:458000], time = 1.59s\n",
      "Loop 205 of 344: train = [408000:458000], test = [458000:460000], time = 1.63s\n",
      "Loop 206 of 344: train = [410000:460000], test = [460000:462000], time = 1.62s\n",
      "Loop 207 of 344: train = [412000:462000], test = [462000:464000], time = 1.69s\n",
      "Loop 208 of 344: train = [414000:464000], test = [464000:466000], time = 1.71s\n",
      "Loop 209 of 344: train = [416000:466000], test = [466000:468000], time = 1.68s\n",
      "Loop 210 of 344: train = [418000:468000], test = [468000:470000], time = 1.68s\n",
      "Loop 211 of 344: train = [420000:470000], test = [470000:472000], time = 1.68s\n",
      "Loop 212 of 344: train = [422000:472000], test = [472000:474000], time = 1.66s\n",
      "Loop 213 of 344: train = [424000:474000], test = [474000:476000], time = 1.71s\n",
      "Loop 214 of 344: train = [426000:476000], test = [476000:478000], time = 1.75s\n",
      "Loop 215 of 344: train = [428000:478000], test = [478000:480000], time = 1.73s\n",
      "Loop 216 of 344: train = [430000:480000], test = [480000:482000], time = 1.73s\n",
      "Loop 217 of 344: train = [432000:482000], test = [482000:484000], time = 1.75s\n",
      "Loop 218 of 344: train = [434000:484000], test = [484000:486000], time = 1.68s\n",
      "Loop 219 of 344: train = [436000:486000], test = [486000:488000], time = 1.71s\n",
      "Loop 220 of 344: train = [438000:488000], test = [488000:490000], time = 1.68s\n",
      "Loop 221 of 344: train = [440000:490000], test = [490000:492000], time = 1.75s\n",
      "Loop 222 of 344: train = [442000:492000], test = [492000:494000], time = 1.69s\n",
      "Loop 223 of 344: train = [444000:494000], test = [494000:496000], time = 1.67s\n",
      "Loop 224 of 344: train = [446000:496000], test = [496000:498000], time = 1.68s\n",
      "Loop 225 of 344: train = [448000:498000], test = [498000:500000], time = 1.65s\n",
      "Loop 226 of 344: train = [450000:500000], test = [500000:502000], time = 1.64s\n",
      "Loop 227 of 344: train = [452000:502000], test = [502000:504000], time = 1.61s\n",
      "Loop 228 of 344: train = [454000:504000], test = [504000:506000], time = 1.59s\n",
      "Loop 229 of 344: train = [456000:506000], test = [506000:508000], time = 1.58s\n",
      "Loop 230 of 344: train = [458000:508000], test = [508000:510000], time = 1.56s\n",
      "Loop 231 of 344: train = [460000:510000], test = [510000:512000], time = 1.53s\n",
      "Loop 232 of 344: train = [462000:512000], test = [512000:514000], time = 1.60s\n",
      "Loop 233 of 344: train = [464000:514000], test = [514000:516000], time = 1.58s\n",
      "Loop 234 of 344: train = [466000:516000], test = [516000:518000], time = 1.70s\n",
      "Loop 235 of 344: train = [468000:518000], test = [518000:520000], time = 1.71s\n",
      "Loop 236 of 344: train = [470000:520000], test = [520000:522000], time = 1.69s\n",
      "Loop 237 of 344: train = [472000:522000], test = [522000:524000], time = 1.73s\n",
      "Loop 238 of 344: train = [474000:524000], test = [524000:526000], time = 1.72s\n",
      "Loop 239 of 344: train = [476000:526000], test = [526000:528000], time = 1.76s\n",
      "Loop 240 of 344: train = [478000:528000], test = [528000:530000], time = 1.73s\n",
      "Loop 241 of 344: train = [480000:530000], test = [530000:532000], time = 1.71s\n",
      "Loop 242 of 344: train = [482000:532000], test = [532000:534000], time = 1.75s\n",
      "Loop 243 of 344: train = [484000:534000], test = [534000:536000], time = 1.73s\n",
      "Loop 244 of 344: train = [486000:536000], test = [536000:538000], time = 1.77s\n",
      "Loop 245 of 344: train = [488000:538000], test = [538000:540000], time = 1.76s\n",
      "Loop 246 of 344: train = [490000:540000], test = [540000:542000], time = 1.86s\n",
      "Loop 247 of 344: train = [492000:542000], test = [542000:544000], time = 1.77s\n",
      "Loop 248 of 344: train = [494000:544000], test = [544000:546000], time = 1.81s\n",
      "Loop 249 of 344: train = [496000:546000], test = [546000:548000], time = 1.82s\n",
      "Loop 250 of 344: train = [498000:548000], test = [548000:550000], time = 1.82s\n",
      "Loop 251 of 344: train = [500000:550000], test = [550000:552000], time = 1.80s\n",
      "Loop 252 of 344: train = [502000:552000], test = [552000:554000], time = 1.82s\n",
      "Loop 253 of 344: train = [504000:554000], test = [554000:556000], time = 1.82s\n",
      "Loop 254 of 344: train = [506000:556000], test = [556000:558000], time = 1.84s\n",
      "Loop 255 of 344: train = [508000:558000], test = [558000:560000], time = 1.77s\n",
      "Loop 256 of 344: train = [510000:560000], test = [560000:562000], time = 1.76s\n",
      "Loop 257 of 344: train = [512000:562000], test = [562000:564000], time = 1.77s\n",
      "Loop 258 of 344: train = [514000:564000], test = [564000:566000], time = 1.76s\n",
      "Loop 259 of 344: train = [516000:566000], test = [566000:568000], time = 1.72s\n",
      "Loop 260 of 344: train = [518000:568000], test = [568000:570000], time = 1.76s\n",
      "Loop 261 of 344: train = [520000:570000], test = [570000:572000], time = 1.79s\n",
      "Loop 262 of 344: train = [522000:572000], test = [572000:574000], time = 1.77s\n",
      "Loop 263 of 344: train = [524000:574000], test = [574000:576000], time = 1.80s\n",
      "Loop 264 of 344: train = [526000:576000], test = [576000:578000], time = 1.78s\n",
      "Loop 265 of 344: train = [528000:578000], test = [578000:580000], time = 1.81s\n",
      "Loop 266 of 344: train = [530000:580000], test = [580000:582000], time = 1.85s\n",
      "Loop 267 of 344: train = [532000:582000], test = [582000:584000], time = 1.80s\n",
      "Loop 268 of 344: train = [534000:584000], test = [584000:586000], time = 1.73s\n",
      "Loop 269 of 344: train = [536000:586000], test = [586000:588000], time = 1.76s\n",
      "Loop 270 of 344: train = [538000:588000], test = [588000:590000], time = 1.72s\n",
      "Loop 271 of 344: train = [540000:590000], test = [590000:592000], time = 1.69s\n",
      "Loop 272 of 344: train = [542000:592000], test = [592000:594000], time = 1.66s\n",
      "Loop 273 of 344: train = [544000:594000], test = [594000:596000], time = 1.75s\n",
      "Loop 274 of 344: train = [546000:596000], test = [596000:598000], time = 1.77s\n",
      "Loop 275 of 344: train = [548000:598000], test = [598000:600000], time = 1.73s\n",
      "Loop 276 of 344: train = [550000:600000], test = [600000:602000], time = 1.63s\n",
      "Loop 277 of 344: train = [552000:602000], test = [602000:604000], time = 1.66s\n",
      "Loop 278 of 344: train = [554000:604000], test = [604000:606000], time = 1.65s\n",
      "Loop 279 of 344: train = [556000:606000], test = [606000:608000], time = 1.65s\n",
      "Loop 280 of 344: train = [558000:608000], test = [608000:610000], time = 1.68s\n",
      "Loop 281 of 344: train = [560000:610000], test = [610000:612000], time = 1.70s\n",
      "Loop 282 of 344: train = [562000:612000], test = [612000:614000], time = 1.67s\n",
      "Loop 283 of 344: train = [564000:614000], test = [614000:616000], time = 1.73s\n",
      "Loop 284 of 344: train = [566000:616000], test = [616000:618000], time = 1.72s\n",
      "Loop 285 of 344: train = [568000:618000], test = [618000:620000], time = 1.70s\n",
      "Loop 286 of 344: train = [570000:620000], test = [620000:622000], time = 1.69s\n",
      "Loop 287 of 344: train = [572000:622000], test = [622000:624000], time = 1.68s\n",
      "Loop 288 of 344: train = [574000:624000], test = [624000:626000], time = 1.68s\n",
      "Loop 289 of 344: train = [576000:626000], test = [626000:628000], time = 1.67s\n",
      "Loop 290 of 344: train = [578000:628000], test = [628000:630000], time = 1.71s\n",
      "Loop 291 of 344: train = [580000:630000], test = [630000:632000], time = 1.71s\n",
      "Loop 292 of 344: train = [582000:632000], test = [632000:634000], time = 1.61s\n",
      "Loop 293 of 344: train = [584000:634000], test = [634000:636000], time = 1.68s\n",
      "Loop 294 of 344: train = [586000:636000], test = [636000:638000], time = 1.66s\n",
      "Loop 295 of 344: train = [588000:638000], test = [638000:640000], time = 1.64s\n",
      "Loop 296 of 344: train = [590000:640000], test = [640000:642000], time = 1.68s\n",
      "Loop 297 of 344: train = [592000:642000], test = [642000:644000], time = 1.70s\n",
      "Loop 298 of 344: train = [594000:644000], test = [644000:646000], time = 1.67s\n",
      "Loop 299 of 344: train = [596000:646000], test = [646000:648000], time = 1.68s\n",
      "Loop 300 of 344: train = [598000:648000], test = [648000:650000], time = 1.69s\n",
      "Loop 301 of 344: train = [600000:650000], test = [650000:652000], time = 1.71s\n",
      "Loop 302 of 344: train = [602000:652000], test = [652000:654000], time = 1.72s\n",
      "Loop 303 of 344: train = [604000:654000], test = [654000:656000], time = 1.70s\n",
      "Loop 304 of 344: train = [606000:656000], test = [656000:658000], time = 1.71s\n",
      "Loop 305 of 344: train = [608000:658000], test = [658000:660000], time = 1.65s\n",
      "Loop 306 of 344: train = [610000:660000], test = [660000:662000], time = 1.68s\n",
      "Loop 307 of 344: train = [612000:662000], test = [662000:664000], time = 1.68s\n",
      "Loop 308 of 344: train = [614000:664000], test = [664000:666000], time = 1.70s\n",
      "Loop 309 of 344: train = [616000:666000], test = [666000:668000], time = 1.71s\n",
      "Loop 310 of 344: train = [618000:668000], test = [668000:670000], time = 1.67s\n",
      "Loop 311 of 344: train = [620000:670000], test = [670000:672000], time = 1.67s\n",
      "Loop 312 of 344: train = [622000:672000], test = [672000:674000], time = 1.64s\n",
      "Loop 313 of 344: train = [624000:674000], test = [674000:676000], time = 1.61s\n",
      "Loop 314 of 344: train = [626000:676000], test = [676000:678000], time = 1.60s\n",
      "Loop 315 of 344: train = [628000:678000], test = [678000:680000], time = 1.52s\n",
      "Loop 316 of 344: train = [630000:680000], test = [680000:682000], time = 1.54s\n",
      "Loop 317 of 344: train = [632000:682000], test = [682000:684000], time = 1.52s\n",
      "Loop 318 of 344: train = [634000:684000], test = [684000:686000], time = 1.48s\n",
      "Loop 319 of 344: train = [636000:686000], test = [686000:688000], time = 1.59s\n",
      "Loop 320 of 344: train = [638000:688000], test = [688000:690000], time = 1.54s\n",
      "Loop 321 of 344: train = [640000:690000], test = [690000:692000], time = 1.55s\n",
      "Loop 322 of 344: train = [642000:692000], test = [692000:694000], time = 1.51s\n",
      "Loop 323 of 344: train = [644000:694000], test = [694000:696000], time = 1.55s\n",
      "Loop 324 of 344: train = [646000:696000], test = [696000:698000], time = 1.54s\n",
      "Loop 325 of 344: train = [648000:698000], test = [698000:700000], time = 1.61s\n",
      "Loop 326 of 344: train = [650000:700000], test = [700000:702000], time = 1.62s\n",
      "Loop 327 of 344: train = [652000:702000], test = [702000:704000], time = 1.63s\n",
      "Loop 328 of 344: train = [654000:704000], test = [704000:706000], time = 1.63s\n",
      "Loop 329 of 344: train = [656000:706000], test = [706000:708000], time = 1.65s\n",
      "Loop 330 of 344: train = [658000:708000], test = [708000:710000], time = 1.68s\n",
      "Loop 331 of 344: train = [660000:710000], test = [710000:712000], time = 1.67s\n",
      "Loop 332 of 344: train = [662000:712000], test = [712000:714000], time = 1.66s\n",
      "Loop 333 of 344: train = [664000:714000], test = [714000:716000], time = 1.64s\n",
      "Loop 334 of 344: train = [666000:716000], test = [716000:718000], time = 1.69s\n",
      "Loop 335 of 344: train = [668000:718000], test = [718000:720000], time = 1.69s\n",
      "Loop 336 of 344: train = [670000:720000], test = [720000:722000], time = 1.70s\n",
      "Loop 337 of 344: train = [672000:722000], test = [722000:724000], time = 1.73s\n",
      "Loop 338 of 344: train = [674000:724000], test = [724000:726000], time = 1.73s\n",
      "Loop 339 of 344: train = [676000:726000], test = [726000:728000], time = 1.71s\n",
      "Loop 340 of 344: train = [678000:728000], test = [728000:730000], time = 1.69s\n",
      "Loop 341 of 344: train = [680000:730000], test = [730000:732000], time = 1.65s\n",
      "Loop 342 of 344: train = [682000:732000], test = [732000:734000], time = 1.64s\n",
      "Loop 343 of 344: train = [684000:734000], test = [734000:736000], time = 1.69s\n",
      "Loop 344 of 344: train = [686000:736000], test = [736000:738000], time = 1.62s\n"
     ]
    }
   ],
   "source": [
    "# Run backtesting for the best model obtained\n",
    "backtest = backtesting(data, model, predictors, target, train_size, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4b8f693b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target_1</th>\n",
       "      <th>Predicted_Target_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>737980</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737981</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737982</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737983</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737984</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737985</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737986</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737987</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737988</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737989</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737990</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737991</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737992</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737993</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737994</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737997</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737999</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Target_1  Predicted_Target_1\n",
       "737980         0                   0\n",
       "737981         0                   0\n",
       "737982         1                   1\n",
       "737983         0                   0\n",
       "737984         1                   1\n",
       "737985         0                   0\n",
       "737986         1                   0\n",
       "737987         0                   0\n",
       "737988         1                   1\n",
       "737989         0                   0\n",
       "737990         0                   0\n",
       "737991         0                   0\n",
       "737992         1                   1\n",
       "737993         1                   0\n",
       "737994         0                   0\n",
       "737995         0                   0\n",
       "737996         0                   0\n",
       "737997         1                   1\n",
       "737998         0                   0\n",
       "737999         0                   1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Overview of the first instances of the predictions\n",
    "backtest.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5dacef19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Results for the EUR/USD machine learning model. \n",
      "------------------------------------------------------------------------------------------------------------- \n",
      "Classification report of the Random Forest Classifier model after Bayesian Optimization and Backtesting. \n",
      "Model training done with a rolling window of 50000 instances for training and 2000 instances for testing. \n",
      "The label (direction) to be used is: 0.\n",
      "The time interval between each trade is: 1 minutes. \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.84      0.72    416078\n",
      "           1       0.48      0.22      0.30    271922\n",
      "\n",
      "    accuracy                           0.60    688000\n",
      "   macro avg       0.55      0.53      0.51    688000\n",
      "weighted avg       0.57      0.60      0.55    688000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_interval = re.search(r\"\\d+\", target).group()\n",
    "\n",
    "# classification report of the best model with backtesting\n",
    "print(f\"Final Results for the {symbol} machine learning model. \\n\"\n",
    "      f\"------------------------------------------------------------------------------------------------------------- \\n\"\n",
    "      f\"Classification report of the {model_name} model after Bayesian Optimization and Backtesting. \\n\"\n",
    "      f\"Model training done with a rolling window of {train_size} instances for training and {test_size} instances for testing. \\n\"\n",
    "      f\"The label (direction) to be used is: {label}.\\n\"\n",
    "      f\"The time interval between each trade is: {time_interval} minutes. \\n\"\n",
    "      )\n",
    "\n",
    "report = classification_report(\n",
    "    y_true = backtest[target],\n",
    "    y_pred = backtest[f\"Predicted_{target}\"],\n",
    "    zero_division = 0,\n",
    ")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc27450",
   "metadata": {},
   "source": [
    "It can be observed that the performance is slightly different from the default and optimized model, as this now considers backtesting through the whole dataset.\n",
    "\n",
    "This is the final model that should be used for stock price direction prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
